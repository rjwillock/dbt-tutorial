[0m12:25:59.010856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD50885750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD52B569D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD5292CDD0>]}


============================== 12:25:59.018863 | 815e3d3f-85ae-46c4-bc44-cd56eb7a1b0c ==============================
[0m12:25:59.018863 [info ] [MainThread]: Running with dbt=1.5.0
[0m12:25:59.020858 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:25:59.023850 [info ] [MainThread]: dbt version: 1.5.0
[0m12:25:59.024848 [info ] [MainThread]: python version: 3.11.3
[0m12:25:59.026841 [info ] [MainThread]: python path: C:\Users\RWillock\AppData\Local\Programs\Python\Python311\python.exe
[0m12:25:59.027839 [info ] [MainThread]: os info: Windows-10-10.0.19044-SP0
[0m12:25:59.029834 [info ] [MainThread]: Using profiles.yml file at C:\Users\RWillock\.dbt\profiles.yml
[0m12:25:59.032828 [info ] [MainThread]: Using dbt_project.yml file at C:\Git\dbt-tutorial\jaffle_shop\dbt_project.yml
[0m12:25:59.033824 [info ] [MainThread]: Configuration:
[0m12:26:01.915538 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:26:01.957842 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:26:01.958679 [info ] [MainThread]: Required dependencies:
[0m12:26:01.959680 [debug] [MainThread]: Executing "git --help"
[0m12:26:02.125283 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:26:02.126308 [debug] [MainThread]: STDERR: "b''"
[0m12:26:02.127309 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:26:02.128275 [info ] [MainThread]: Connection:
[0m12:26:02.129272 [info ] [MainThread]:   host: adb-8911032177139987.7.azuredatabricks.net
[0m12:26:02.130270 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/4951d4d476e6bfe3
[0m12:26:02.132266 [info ] [MainThread]:   schema: test_rw
[0m12:26:02.133262 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m12:26:02.135257 [debug] [MainThread]: Using databricks connection "debug"
[0m12:26:02.139249 [debug] [MainThread]: On debug: select 1 as id
[0m12:26:02.140247 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:26:02.836704 [debug] [MainThread]: SQL status: OK in 0.699999988079071 seconds
[0m12:26:02.838700 [debug] [MainThread]: On debug: Close
[0m12:26:03.022548 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:26:03.023574 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:26:03.025572 [debug] [MainThread]: Command `dbt debug` succeeded at 12:26:03.025572 after 4.12 seconds
[0m12:26:03.025572 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:26:03.026570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD52F14150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD4E04F550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD4E04F510>]}
[0m12:26:03.027569 [debug] [MainThread]: Flushing usage events
[0m13:07:17.811882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D723EE790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D723EF810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D723036D0>]}


============================== 13:07:17.816895 | 88bc0659-2b44-4f87-b174-0c88b036ce32 ==============================
[0m13:07:17.816895 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:07:17.817568 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:07:19.726029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D72303890>]}
[0m13:07:19.749976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D03B8C510>]}
[0m13:07:19.780848 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:07:19.783840 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:07:19.785835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D03B66150>]}
[0m13:07:21.283116 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m13:07:21.302104 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m13:07:21.477786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D719B13D0>]}
[0m13:07:21.495736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D03DC2E90>]}
[0m13:07:21.496734 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:07:21.497730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D03DC3FD0>]}
[0m13:07:21.499725 [info ] [MainThread]: 
[0m13:07:21.501755 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:07:21.505710 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:07:21.523663 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:07:21.527656 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:07:21.528664 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:07:22.890416 [debug] [ThreadPool]: SQL status: OK in 1.3600000143051147 seconds
[0m13:07:22.920855 [debug] [ThreadPool]: On list_schemas: Close
[0m13:07:23.092567 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m13:07:23.103538 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:23.104536 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:07:23.105533 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m13:07:23.105533 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:07:23.849435 [debug] [ThreadPool]: SQL status: OK in 0.7400000095367432 seconds
[0m13:07:23.861201 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:07:23.863163 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m13:07:24.272482 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m13:07:24.276540 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m13:07:24.277540 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:07:24.277540 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m13:07:24.436921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D70307250>]}
[0m13:07:24.436921 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:24.437928 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:07:24.438917 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:07:24.439916 [info ] [MainThread]: 
[0m13:07:24.504791 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:07:24.506789 [info ] [Thread-1 (]: 1 of 2 START sql table model test_rw.my_first_dbt_model ........................ [RUN]
[0m13:07:24.511774 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.my_first_dbt_model'
[0m13:07:24.515773 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:07:24.522743 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:07:24.539700 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:07:24.517760 => 13:07:24.538702
[0m13:07:24.543721 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:07:24.680322 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:07:24.686305 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:24.687303 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:07:24.688301 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:07:24.690294 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:07:47.295621 [debug] [Thread-1 (]: SQL status: OK in 22.610000610351562 seconds
[0m13:07:47.531631 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:07:24.546679 => 13:07:47.531631
[0m13:07:47.532628 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:07:47.532628 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:07:47.533587 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:07:47.690499 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D03D28090>]}
[0m13:07:47.691495 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test_rw.my_first_dbt_model ................... [[32mOK[0m in 23.18s]
[0m13:07:47.693513 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:07:47.694510 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:07:47.695507 [info ] [Thread-1 (]: 2 of 2 START sql view model test_rw.my_second_dbt_model ........................ [RUN]
[0m13:07:47.699502 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m13:07:47.700495 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:07:47.704484 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:07:47.707476 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:07:47.701492 => 13:07:47.706478
[0m13:07:47.708760 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:07:47.732409 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:07:47.735403 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:47.736397 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:07:47.737399 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw`.`my_first_dbt_model`
where id = 1

[0m13:07:47.738394 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:07:49.061214 [debug] [Thread-1 (]: SQL status: OK in 1.3200000524520874 seconds
[0m13:07:49.066202 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:07:47.710469 => 13:07:49.065204
[0m13:07:49.067197 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:07:49.068195 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:07:49.069192 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:07:49.242578 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88bc0659-2b44-4f87-b174-0c88b036ce32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D03B8E8D0>]}
[0m13:07:49.244569 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test_rw.my_second_dbt_model ................... [[32mOK[0m in 1.54s]
[0m13:07:49.245569 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:07:49.248561 [debug] [MainThread]: On master: ROLLBACK
[0m13:07:49.250555 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:07:49.461298 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:07:49.462326 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:49.462326 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:07:49.463344 [debug] [MainThread]: On master: ROLLBACK
[0m13:07:49.463344 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:07:49.464336 [debug] [MainThread]: On master: Close
[0m13:07:49.626011 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:07:49.626011 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:07:49.627009 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m13:07:49.627009 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:07:49.629003 [info ] [MainThread]: 
[0m13:07:49.629562 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 28.13 seconds (28.13s).
[0m13:07:49.631560 [debug] [MainThread]: Command end result
[0m13:07:49.651511 [info ] [MainThread]: 
[0m13:07:49.652504 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:07:49.654497 [info ] [MainThread]: 
[0m13:07:49.655497 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:07:49.656492 [debug] [MainThread]: Command `dbt run` succeeded at 13:07:49.656492 after 31.91 seconds
[0m13:07:49.657489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D723036D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D723C4150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D6D5FF510>]}
[0m13:07:49.660487 [debug] [MainThread]: Flushing usage events
[0m13:18:34.920548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D51A9090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D592FC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D526A550>]}


============================== 13:18:34.926568 | 917aa5a6-0225-49e0-8071-21290c854060 ==============================
[0m13:18:34.926568 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:18:34.928531 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:18:37.474770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D51A8E90>]}
[0m13:18:37.501670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278E6C06E90>]}
[0m13:18:37.532587 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:18:37.745051 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:18:37.747015 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\example\my_first_dbt_model.sql
[0m13:18:37.791895 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m13:18:37.930526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278E6C5F250>]}
[0m13:18:37.953467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278E6D09390>]}
[0m13:18:37.954465 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:18:37.956698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278E6D094D0>]}
[0m13:18:37.960205 [info ] [MainThread]: 
[0m13:18:37.964194 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:18:37.970176 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:18:38.010076 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:18:38.011068 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:18:38.013063 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:38.848837 [debug] [ThreadPool]: SQL status: OK in 0.8399999737739563 seconds
[0m13:18:38.858810 [debug] [ThreadPool]: On list_schemas: Close
[0m13:18:39.027396 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m13:18:39.041324 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:18:39.042324 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:18:39.043320 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m13:18:39.044345 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:39.637553 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m13:18:39.653508 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:18:39.654506 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m13:18:40.055437 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m13:18:40.060424 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m13:18:40.061421 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:18:40.061421 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m13:18:40.227977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D4AA3810>]}
[0m13:18:40.228975 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:18:40.229972 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:18:40.231967 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:18:40.233962 [info ] [MainThread]: 
[0m13:18:40.240946 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:18:40.242939 [info ] [Thread-1 (]: 1 of 2 START sql table model test_rw.my_first_dbt_model ........................ [RUN]
[0m13:18:40.244934 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.my_first_dbt_model'
[0m13:18:40.245930 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:18:40.250917 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:18:40.254906 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:18:40.246928 => 13:18:40.253908
[0m13:18:40.256901 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:18:40.296794 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:18:40.297791 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:18:40.303790 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw`.`my_first_dbt_model`
  
[0m13:18:40.305775 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:18:40.975984 [debug] [Thread-1 (]: SQL status: OK in 0.6700000166893005 seconds
[0m13:18:41.055772 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:18:41.057768 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:18:41.059761 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below



*/

with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:18:47.233796 [debug] [Thread-1 (]: SQL status: OK in 6.170000076293945 seconds
[0m13:18:47.508064 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:18:40.257898 => 13:18:47.508064
[0m13:18:47.509061 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:18:47.511056 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:18:47.512053 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:18:47.676615 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278E6D21750>]}
[0m13:18:47.678610 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test_rw.my_first_dbt_model ................... [[32mOK[0m in 7.43s]
[0m13:18:47.681602 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:18:47.684594 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:18:47.688595 [info ] [Thread-1 (]: 2 of 2 START sql view model test_rw.my_second_dbt_model ........................ [RUN]
[0m13:18:47.692572 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m13:18:47.693570 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:18:47.700232 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:18:47.708545 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:18:47.694568 => 13:18:47.706549
[0m13:18:47.711523 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:18:47.765378 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:18:47.774356 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:18:47.775352 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:18:47.776349 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw`.`my_first_dbt_model`
where id = 1

[0m13:18:47.778344 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:18:49.553383 [debug] [Thread-1 (]: SQL status: OK in 1.7799999713897705 seconds
[0m13:18:49.557370 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:18:47.712523 => 13:18:49.557370
[0m13:18:49.558367 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:18:49.559365 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:18:49.560362 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:18:49.725973 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '917aa5a6-0225-49e0-8071-21290c854060', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278E6C70F10>]}
[0m13:18:49.727970 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test_rw.my_second_dbt_model ................... [[32mOK[0m in 2.04s]
[0m13:18:49.730964 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:18:49.733953 [debug] [MainThread]: On master: ROLLBACK
[0m13:18:49.734950 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:18:49.979301 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:18:49.980295 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:18:49.981292 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:18:49.982290 [debug] [MainThread]: On master: ROLLBACK
[0m13:18:49.984288 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:18:49.985282 [debug] [MainThread]: On master: Close
[0m13:18:50.149844 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:18:50.151865 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:18:50.152864 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m13:18:50.153862 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:18:50.155830 [info ] [MainThread]: 
[0m13:18:50.156825 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 12.19 seconds (12.19s).
[0m13:18:50.159820 [debug] [MainThread]: Command end result
[0m13:18:50.184760 [info ] [MainThread]: 
[0m13:18:50.187756 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:18:50.191734 [info ] [MainThread]: 
[0m13:18:50.192730 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:18:50.195722 [debug] [MainThread]: Command `dbt run` succeeded at 13:18:50.194725 after 15.36 seconds
[0m13:18:50.196719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D5772B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D06BF550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000278D06BF510>]}
[0m13:18:50.196719 [debug] [MainThread]: Flushing usage events
[0m13:21:09.569395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7DD541A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7DD7E9FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7DD541410>]}


============================== 13:21:09.578371 | 4600f754-f809-49d2-b851-4e1c30e3d7f2 ==============================
[0m13:21:09.578371 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:21:09.580366 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:21:12.631243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7DDC86B10>]}
[0m13:21:12.693081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7EEF8CD90>]}
[0m13:21:12.793811 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:21:13.235632 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:21:13.238625 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\example\my_first_dbt_model.sql
[0m13:21:13.314423 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m13:21:13.498933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7EF17EE50>]}
[0m13:21:13.534835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7EF060650>]}
[0m13:21:13.536847 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:21:13.542813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7EF063610>]}
[0m13:21:13.549795 [info ] [MainThread]: 
[0m13:21:13.562764 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:21:13.577735 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:21:13.633572 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:21:13.636564 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:21:13.638559 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:21:14.583162 [debug] [ThreadPool]: SQL status: OK in 0.9399999976158142 seconds
[0m13:21:14.595128 [debug] [ThreadPool]: On list_schemas: Close
[0m13:21:14.771687 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m13:21:14.791605 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:21:14.792632 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:21:14.793599 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m13:21:14.794596 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:21:15.425398 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m13:21:15.442354 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:21:15.443351 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m13:21:15.837222 [debug] [ThreadPool]: SQL status: OK in 0.38999998569488525 seconds
[0m13:21:15.842236 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m13:21:15.843233 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:21:15.844231 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m13:21:16.022727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7EF0B9210>]}
[0m13:21:16.023723 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:21:16.023723 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:21:16.025717 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:21:16.026715 [info ] [MainThread]: 
[0m13:21:16.034694 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:21:16.036691 [info ] [Thread-1 (]: 1 of 2 START sql view model test_rw.my_first_dbt_model ......................... [RUN]
[0m13:21:16.043681 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.my_first_dbt_model'
[0m13:21:16.045666 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:21:16.048657 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:21:16.052646 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:21:16.046663 => 13:21:16.051650
[0m13:21:16.055641 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:21:16.094547 [debug] [Thread-1 (]: Dropping relation `test_rw`.`my_first_dbt_model` because it is of type table
[0m13:21:16.110497 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:21:16.113490 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */
drop table if exists `test_rw`.`my_first_dbt_model`
[0m13:21:16.115480 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:21:17.383791 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m13:21:17.411716 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:21:17.413714 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:21:17.414709 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:21:17.415707 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */
create or replace view `test_rw`.`my_first_dbt_model`
  
  
  as
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below


*/

with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null

[0m13:21:18.160283 [debug] [Thread-1 (]: SQL status: OK in 0.7400000095367432 seconds
[0m13:21:18.175242 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:21:16.056638 => 13:21:18.175242
[0m13:21:18.176240 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:21:18.177237 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:21:18.177237 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:21:18.338063 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7EEFC5890>]}
[0m13:21:18.339063 [info ] [Thread-1 (]: 1 of 2 OK created sql view model test_rw.my_first_dbt_model .................... [[32mOK[0m in 2.30s]
[0m13:21:18.341056 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:21:18.342053 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:21:18.343052 [info ] [Thread-1 (]: 2 of 2 START sql view model test_rw.my_second_dbt_model ........................ [RUN]
[0m13:21:18.345046 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m13:21:18.346043 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:21:18.351029 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:21:18.359038 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:21:18.346043 => 13:21:18.358012
[0m13:21:18.360033 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:21:18.369018 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:21:18.375963 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:21:18.377958 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:21:18.379952 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw`.`my_first_dbt_model`
where id = 1

[0m13:21:18.380953 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:21:20.100369 [debug] [Thread-1 (]: SQL status: OK in 1.7200000286102295 seconds
[0m13:21:20.108385 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:21:18.361004 => 13:21:20.108385
[0m13:21:20.110340 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:21:20.110340 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:21:20.111338 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:21:20.280887 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4600f754-f809-49d2-b851-4e1c30e3d7f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7EF051DD0>]}
[0m13:21:20.282884 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test_rw.my_second_dbt_model ................... [[32mOK[0m in 1.94s]
[0m13:21:20.284876 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:21:20.287868 [debug] [MainThread]: On master: ROLLBACK
[0m13:21:20.288864 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:21:20.517256 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:21:20.518253 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:21:20.519250 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:21:20.520249 [debug] [MainThread]: On master: ROLLBACK
[0m13:21:20.521245 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:21:20.522242 [debug] [MainThread]: On master: Close
[0m13:21:20.683812 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:21:20.684809 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:21:20.685806 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m13:21:20.686804 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:21:20.689795 [info ] [MainThread]: 
[0m13:21:20.690792 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 7.13 seconds (7.13s).
[0m13:21:20.692789 [debug] [MainThread]: Command end result
[0m13:21:20.715727 [info ] [MainThread]: 
[0m13:21:20.716725 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:21:20.718720 [info ] [MainThread]: 
[0m13:21:20.722715 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:21:20.727696 [debug] [MainThread]: Command `dbt run` succeeded at 13:21:20.727696 after 11.26 seconds
[0m13:21:20.729696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7DD5C9210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7D8A0F550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7D8A0F510>]}
[0m13:21:20.730689 [debug] [MainThread]: Flushing usage events
[0m13:42:20.586784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F42AD2F010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F42AD23350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F42B47FAD0>]}


============================== 13:42:20.591805 | a96e602d-adc7-4a02-a2d3-e0d2787d1df9 ==============================
[0m13:42:20.591805 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:42:20.593768 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:42:21.909936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F42B0ABD90>]}
[0m13:42:21.932875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F43C766E90>]}
[0m13:42:21.961799 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:42:22.129349 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 3 files changed.
[0m13:42:22.130346 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\example\customers.sql
[0m13:42:22.131344 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\example\schema.yml
[0m13:42:22.132340 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\example\my_first_dbt_model.sql
[0m13:42:22.133337 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\example\my_second_dbt_model.sql
[0m13:42:22.159269 [debug] [MainThread]: 1699: static parser successfully parsed example\customers.sql
[0m13:42:22.176261 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m13:42:22.181248 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m13:42:22.252020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F43C7715D0>]}
[0m13:42:22.266980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F43C8615D0>]}
[0m13:42:22.267977 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:42:22.268779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F42B4ACCD0>]}
[0m13:42:22.270810 [info ] [MainThread]: 
[0m13:42:22.272772 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:42:22.276761 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:42:22.293756 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:42:22.294740 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:42:22.295746 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:42:23.050634 [debug] [ThreadPool]: SQL status: OK in 0.75 seconds
[0m13:42:23.057617 [debug] [ThreadPool]: On list_schemas: Close
[0m13:42:23.219641 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m13:42:23.229654 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:42:23.229654 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:42:23.230655 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m13:42:23.230655 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:42:23.808379 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m13:42:23.817555 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m13:42:23.818556 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m13:42:24.274642 [debug] [ThreadPool]: SQL status: OK in 0.46000000834465027 seconds
[0m13:42:24.278633 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m13:42:24.279629 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:42:24.279629 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m13:42:24.464316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F43C970A90>]}
[0m13:42:24.464316 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:42:24.465354 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:42:24.466350 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:42:24.467310 [info ] [MainThread]: 
[0m13:42:24.476287 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m13:42:24.477289 [info ] [Thread-1 (]: 1 of 3 START sql view model test_rw.customers .................................. [RUN]
[0m13:42:24.482270 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m13:42:24.483267 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m13:42:24.485262 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m13:42:24.491290 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 13:42:24.484264 => 13:42:24.490296
[0m13:42:24.493293 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m13:42:24.535202 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m13:42:24.539205 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:42:24.541197 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m13:42:24.543220 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
create or replace view `test_rw`.`customers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:42:24.544190 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:42:25.071833 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
create or replace view `test_rw`.`customers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:42:25.072792 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:42:25.073790 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:609)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:501)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:361)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:339)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:324)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:373)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:586)
	... 20 more

[0m13:42:25.074821 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x01\xed\xf5yp\xb1\x10\xbc\xae\x1eW\x02S\xcb\xc3b'
[0m13:42:25.075823 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 13:42:24.494310 => 13:42:25.075823
[0m13:42:25.075823 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m13:42:25.076784 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:42:25.077781 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m13:42:25.257654 [debug] [Thread-1 (]: Runtime Error in model customers (models\example\customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:42:25.258653 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F43C99D850>]}
[0m13:42:25.259615 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model test_rw.customers ......................... [[31mERROR[0m in 0.78s]
[0m13:42:25.260494 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m13:42:25.261495 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:42:25.262491 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw.my_first_dbt_model ........................ [RUN]
[0m13:42:25.264487 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now model.jaffle_shop.my_first_dbt_model)
[0m13:42:25.266481 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:42:25.269506 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:42:25.271503 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:42:25.266481 => 13:42:25.271503
[0m13:42:25.272466 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:42:25.303383 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:42:25.303383 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:42:25.304407 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw`.`my_first_dbt_model`
  
[0m13:42:25.306374 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:26.070313 [debug] [Thread-1 (]: SQL status: OK in 0.7599999904632568 seconds
[0m13:42:26.079265 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:42:26.080263 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */
drop view if exists `test_rw`.`my_first_dbt_model`
[0m13:42:26.937807 [debug] [Thread-1 (]: SQL status: OK in 0.8600000143051147 seconds
[0m13:42:26.972689 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:42:26.974714 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:42:26.974714 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:42:30.804261 [debug] [Thread-1 (]: SQL status: OK in 3.8299999237060547 seconds
[0m13:42:30.840166 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:42:25.274463 => 13:42:30.840166
[0m13:42:30.841163 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:42:30.842161 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:42:30.842161 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:42:30.999191 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F43C97B790>]}
[0m13:42:31.000191 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw.my_first_dbt_model ................... [[32mOK[0m in 5.74s]
[0m13:42:31.002185 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:42:31.003184 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:42:31.004181 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw.my_second_dbt_model ........................ [RUN]
[0m13:42:31.006176 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m13:42:31.007206 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:42:31.012159 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:42:31.016183 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:42:31.007206 => 13:42:31.016183
[0m13:42:31.017196 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:42:31.022836 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:42:31.027125 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:42:31.028118 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:42:31.029116 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw`.`my_first_dbt_model`
where id = 1

[0m13:42:31.030113 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:32.441150 [debug] [Thread-1 (]: SQL status: OK in 1.409999966621399 seconds
[0m13:42:32.444178 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:42:31.017196 => 13:42:32.444178
[0m13:42:32.445179 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:42:32.445179 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:42:32.446137 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:42:32.609624 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a96e602d-adc7-4a02-a2d3-e0d2787d1df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F43CED5A10>]}
[0m13:42:32.609624 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw.my_second_dbt_model ................... [[32mOK[0m in 1.60s]
[0m13:42:32.611590 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:42:32.613585 [debug] [MainThread]: On master: ROLLBACK
[0m13:42:32.614582 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:42:32.818628 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:42:32.819656 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:42:32.819656 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:42:32.820650 [debug] [MainThread]: On master: ROLLBACK
[0m13:42:32.820650 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:42:32.821656 [debug] [MainThread]: On master: Close
[0m13:42:32.978082 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:42:32.979123 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:42:32.980079 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m13:42:32.980079 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:42:32.981076 [info ] [MainThread]: 
[0m13:42:32.982107 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 10.71 seconds (10.71s).
[0m13:42:32.984070 [debug] [MainThread]: Command end result
[0m13:42:33.072831 [info ] [MainThread]: 
[0m13:42:33.074826 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:42:33.076820 [info ] [MainThread]: 
[0m13:42:33.079823 [error] [MainThread]: [33mRuntime Error in model customers (models\example\customers.sql)[0m
[0m13:42:33.081807 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m13:42:33.082879 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m13:42:33.083881 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:42:33.085874 [info ] [MainThread]: 
[0m13:42:33.086871 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m13:42:33.087869 [debug] [MainThread]: Command `dbt run` failed at 13:42:33.087869 after 12.55 seconds
[0m13:42:33.090861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F42AC759D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F4263A9650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F42AD23350>]}
[0m13:42:33.091860 [debug] [MainThread]: Flushing usage events
[0m13:48:59.673627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002338EC72FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002338EF608D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002338EC836D0>]}


============================== 13:48:59.681594 | 794397dc-7dd1-41ab-bb7f-340732120c4b ==============================
[0m13:48:59.681594 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:48:59.683553 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:49:01.621735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002338CDA4DD0>]}
[0m13:49:01.644705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A06A4A90>]}
[0m13:49:01.673594 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:49:01.690551 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:49:01.692544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A07F6990>]}
[0m13:49:03.157670 [debug] [MainThread]: 1699: static parser successfully parsed example\customers.sql
[0m13:49:03.177601 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m13:49:03.183584 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m13:49:03.300271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A07E6590>]}
[0m13:49:03.317191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A06D9450>]}
[0m13:49:03.317191 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:49:03.318507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A06F5690>]}
[0m13:49:03.320540 [info ] [MainThread]: 
[0m13:49:03.322500 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:49:03.326492 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:49:03.341483 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:49:03.342447 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:49:03.345444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:49:04.115724 [debug] [ThreadPool]: SQL status: OK in 0.7699999809265137 seconds
[0m13:49:04.121270 [debug] [ThreadPool]: On list_schemas: Close
[0m13:49:04.317339 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test_rw_test_rw)
[0m13:49:04.319333 [debug] [ThreadPool]: Creating schema "schema: "test_rw_test_rw"
"
[0m13:49:04.325318 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:04.326315 [debug] [ThreadPool]: Using databricks connection "create__test_rw_test_rw"
[0m13:49:04.327313 [debug] [ThreadPool]: On create__test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "create__test_rw_test_rw"} */
create schema if not exists `test_rw_test_rw`
  
[0m13:49:04.327313 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:49:05.246204 [debug] [ThreadPool]: SQL status: OK in 0.9200000166893005 seconds
[0m13:49:05.248205 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:49:05.249196 [debug] [ThreadPool]: On create__test_rw_test_rw: ROLLBACK
[0m13:49:05.250193 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:49:05.251191 [debug] [ThreadPool]: On create__test_rw_test_rw: Close
[0m13:49:05.428912 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw_test_rw'
[0m13:49:05.440882 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:05.441879 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m13:49:05.442877 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show tables in `test_rw_test_rw`
  
[0m13:49:05.442877 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:49:05.963286 [debug] [ThreadPool]: SQL status: OK in 0.5199999809265137 seconds
[0m13:49:05.966554 [debug] [ThreadPool]: On list_None_test_rw_test_rw: ROLLBACK
[0m13:49:05.966554 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:49:05.967510 [debug] [ThreadPool]: On list_None_test_rw_test_rw: Close
[0m13:49:06.134308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A0877F50>]}
[0m13:49:06.135347 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:06.135347 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:49:06.136343 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:49:06.137088 [info ] [MainThread]: 
[0m13:49:06.145076 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m13:49:06.150065 [info ] [Thread-1 (]: 1 of 3 START sql view model test_rw_test_rw.customers .......................... [RUN]
[0m13:49:06.155042 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m13:49:06.156040 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m13:49:06.164021 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m13:49:06.168009 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 13:49:06.157037 => 13:49:06.167013
[0m13:49:06.170002 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m13:49:06.265745 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m13:49:06.267740 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:06.268737 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m13:49:06.269735 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
create or replace view `test_rw_test_rw`.`customers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:49:06.270733 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:49:06.766547 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
create or replace view `test_rw_test_rw`.`customers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:49:06.767542 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:49:06.768538 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:609)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:501)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:361)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:339)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:324)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:373)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:586)
	... 20 more

[0m13:49:06.769501 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x01\xed\xf5z`&\x1c\x16\x81\xaf\xd1b\xee\xc0*_'
[0m13:49:06.770499 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 13:49:06.171000 => 13:49:06.770499
[0m13:49:06.771496 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m13:49:06.772493 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:49:06.772493 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m13:49:06.944179 [debug] [Thread-1 (]: Runtime Error in model customers (models\example\customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:49:06.945177 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A0904590>]}
[0m13:49:06.946140 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model test_rw_test_rw.customers ................. [[31mERROR[0m in 0.79s]
[0m13:49:06.947137 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m13:49:06.948135 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:49:06.949131 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw_test_rw.my_first_dbt_model ................ [RUN]
[0m13:49:06.951129 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now model.jaffle_shop.my_first_dbt_model)
[0m13:49:06.952126 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:49:06.955120 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:49:06.959108 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:49:06.952126 => 13:49:06.959108
[0m13:49:06.961109 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:49:07.035901 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:49:07.037896 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:07.038893 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:49:07.038893 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw_test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:49:07.039891 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:49:11.961827 [debug] [Thread-1 (]: SQL status: OK in 4.920000076293945 seconds
[0m13:49:12.017679 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:49:06.962129 => 13:49:12.016680
[0m13:49:12.018675 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:49:12.020670 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:49:12.021684 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:49:12.183234 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A0730A90>]}
[0m13:49:12.184232 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw_test_rw.my_first_dbt_model ........... [[32mOK[0m in 5.23s]
[0m13:49:12.186229 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:49:12.188255 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:49:12.189219 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw_test_rw.my_second_dbt_model ................ [RUN]
[0m13:49:12.191214 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m13:49:12.192245 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:49:12.196237 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:49:12.198233 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:49:12.192245 => 13:49:12.198233
[0m13:49:12.199206 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:49:12.206212 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:49:12.208168 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:12.209177 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:49:12.212160 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw_test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw_test_rw`.`my_first_dbt_model`
where id = 1

[0m13:49:12.213202 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:49:13.258855 [debug] [Thread-1 (]: SQL status: OK in 1.0399999618530273 seconds
[0m13:49:13.261874 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:49:12.200191 => 13:49:13.260885
[0m13:49:13.261874 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:49:13.262846 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:49:13.263882 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:49:13.419198 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '794397dc-7dd1-41ab-bb7f-340732120c4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233A0716B90>]}
[0m13:49:13.420194 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw_test_rw.my_second_dbt_model ........... [[32mOK[0m in 1.23s]
[0m13:49:13.420923 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:49:13.422919 [debug] [MainThread]: On master: ROLLBACK
[0m13:49:13.423951 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:49:13.622920 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:49:13.623920 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:13.623920 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:49:13.624925 [debug] [MainThread]: On master: ROLLBACK
[0m13:49:13.624925 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:49:13.625913 [debug] [MainThread]: On master: Close
[0m13:49:13.785357 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:49:13.786357 [debug] [MainThread]: Connection 'create__test_rw_test_rw' was properly closed.
[0m13:49:13.786357 [debug] [MainThread]: Connection 'list_None_test_rw_test_rw' was properly closed.
[0m13:49:13.787354 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:49:13.788351 [info ] [MainThread]: 
[0m13:49:13.789421 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 10.47 seconds (10.47s).
[0m13:49:13.791421 [debug] [MainThread]: Command end result
[0m13:49:13.814359 [info ] [MainThread]: 
[0m13:49:13.817350 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:49:13.818346 [info ] [MainThread]: 
[0m13:49:13.819343 [error] [MainThread]: [33mRuntime Error in model customers (models\example\customers.sql)[0m
[0m13:49:13.821339 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m13:49:13.822336 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m13:49:13.823333 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:49:13.824331 [info ] [MainThread]: 
[0m13:49:13.824941 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m13:49:13.828934 [debug] [MainThread]: Command `dbt run` failed at 13:49:13.827943 after 14.22 seconds
[0m13:49:13.831931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002338EFDFF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002338F3ABD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002338A2DBE90>]}
[0m13:49:13.833926 [debug] [MainThread]: Flushing usage events
[0m13:55:04.114764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDC2DF510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDC2DEF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDC2B9090>]}


============================== 13:55:04.120751 | 4ebbb149-2778-4c54-813c-2d99aaa036e0 ==============================
[0m13:55:04.120751 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:55:04.121714 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:55:06.380212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDC368E10>]}
[0m13:55:06.405146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BEDD15990>]}
[0m13:55:06.432074 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:55:06.621570 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m13:55:06.622568 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\example\xcustomers.sql
[0m13:55:06.623565 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models\example\customers.sql
[0m13:55:06.658472 [debug] [MainThread]: 1699: static parser successfully parsed example\xcustomers.sql
[0m13:55:06.700359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BEDEF4750>]}
[0m13:55:06.722301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BEDDF2F90>]}
[0m13:55:06.723300 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:55:06.724296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BEDDF3FD0>]}
[0m13:55:06.727288 [info ] [MainThread]: 
[0m13:55:06.729283 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:55:06.733273 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:55:06.754251 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:55:06.755244 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:55:06.756213 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:55:07.535619 [debug] [ThreadPool]: SQL status: OK in 0.7799999713897705 seconds
[0m13:55:07.543597 [debug] [ThreadPool]: On list_schemas: Close
[0m13:55:07.722084 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw_test_rw'
[0m13:55:07.734091 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:55:07.735049 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m13:55:07.736085 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show tables in `test_rw_test_rw`
  
[0m13:55:07.737081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:55:08.331501 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m13:55:08.342432 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m13:55:08.342432 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show views in `test_rw_test_rw`
  
[0m13:55:08.773327 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m13:55:08.778274 [debug] [ThreadPool]: On list_None_test_rw_test_rw: ROLLBACK
[0m13:55:08.779272 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:55:08.779272 [debug] [ThreadPool]: On list_None_test_rw_test_rw: Close
[0m13:55:08.976744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDBBB3810>]}
[0m13:55:08.976744 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:55:08.977741 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:55:08.978740 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:55:08.979737 [info ] [MainThread]: 
[0m13:55:08.990710 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:55:08.994706 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw_test_rw.my_first_dbt_model ................ [RUN]
[0m13:55:08.997725 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.my_first_dbt_model'
[0m13:55:08.997725 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:55:09.003410 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:55:09.008390 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:55:08.998687 => 13:55:09.008390
[0m13:55:09.010384 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:55:09.040714 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:55:09.041709 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:55:09.042707 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw_test_rw`.`my_first_dbt_model`
  
[0m13:55:09.043703 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:55:09.699740 [debug] [Thread-1 (]: SQL status: OK in 0.6600000262260437 seconds
[0m13:55:09.770554 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:55:09.772550 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:55:09.773547 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw_test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:55:12.306895 [debug] [Thread-1 (]: SQL status: OK in 2.5299999713897705 seconds
[0m13:55:12.344834 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:55:09.011383 => 13:55:12.344834
[0m13:55:12.345831 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:55:12.346790 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:55:12.347786 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:55:12.505409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BEDF60B50>]}
[0m13:55:12.506408 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw_test_rw.my_first_dbt_model ........... [[32mOK[0m in 3.51s]
[0m13:55:12.508359 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:55:12.509357 [debug] [Thread-1 (]: Began running node model.jaffle_shop.xcustomers
[0m13:55:12.510355 [info ] [Thread-1 (]: 2 of 3 START sql view model test_rw_test_rw.xcustomers ......................... [RUN]
[0m13:55:12.512350 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.xcustomers)
[0m13:55:12.513374 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.xcustomers
[0m13:55:12.515341 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.xcustomers"
[0m13:55:12.521326 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.xcustomers (compile): 13:55:12.513374 => 13:55:12.520329
[0m13:55:12.522322 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.xcustomers
[0m13:55:12.549251 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.xcustomers"
[0m13:55:12.554239 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:55:12.555235 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.xcustomers"
[0m13:55:12.556233 [debug] [Thread-1 (]: On model.jaffle_shop.xcustomers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.xcustomers"} */
create or replace view `test_rw_test_rw`.`xcustomers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:55:12.558228 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:55:13.031910 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.xcustomers"} */
create or replace view `test_rw_test_rw`.`xcustomers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:55:13.032907 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:55:13.033905 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:609)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:501)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:361)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:339)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:324)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:373)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:586)
	... 20 more

[0m13:55:13.034900 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x01\xed\xf5{:u\x18\xb2\xb3\xa7\n\xa2+\x8e\x03W'
[0m13:55:13.035898 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.xcustomers (execute): 13:55:12.523319 => 13:55:13.035898
[0m13:55:13.036934 [debug] [Thread-1 (]: On model.jaffle_shop.xcustomers: ROLLBACK
[0m13:55:13.037893 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:55:13.037893 [debug] [Thread-1 (]: On model.jaffle_shop.xcustomers: Close
[0m13:55:13.222402 [debug] [Thread-1 (]: Runtime Error in model xcustomers (models\example\xcustomers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:55:13.223399 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BEDEF7D10>]}
[0m13:55:13.224397 [error] [Thread-1 (]: 2 of 3 ERROR creating sql view model test_rw_test_rw.xcustomers ................ [[31mERROR[0m in 0.71s]
[0m13:55:13.226392 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.xcustomers
[0m13:55:13.227388 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:55:13.227388 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw_test_rw.my_second_dbt_model ................ [RUN]
[0m13:55:13.229385 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.xcustomers, now model.jaffle_shop.my_second_dbt_model)
[0m13:55:13.230382 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:55:13.239358 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:55:13.241385 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:55:13.231379 => 13:55:13.241385
[0m13:55:13.242351 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:55:13.248332 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:55:13.251325 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:55:13.254317 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:55:13.255317 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw_test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw_test_rw`.`my_first_dbt_model`
where id = 1

[0m13:55:13.257312 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:55:14.690551 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m13:55:14.694543 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:55:13.243347 => 13:55:14.693542
[0m13:55:14.694543 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:55:14.695499 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:55:14.696543 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:55:14.873028 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ebbb149-2778-4c54-813c-2d99aaa036e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BEDEB6B50>]}
[0m13:55:14.874026 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw_test_rw.my_second_dbt_model ........... [[32mOK[0m in 1.64s]
[0m13:55:14.875020 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:55:14.877017 [debug] [MainThread]: On master: ROLLBACK
[0m13:55:14.878049 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:55:15.102287 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:55:15.103283 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:55:15.104280 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:55:15.104280 [debug] [MainThread]: On master: ROLLBACK
[0m13:55:15.105277 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:55:15.105277 [debug] [MainThread]: On master: Close
[0m13:55:15.271835 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:55:15.272832 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:55:15.273829 [debug] [MainThread]: Connection 'list_None_test_rw_test_rw' was properly closed.
[0m13:55:15.273829 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:55:15.275824 [info ] [MainThread]: 
[0m13:55:15.276821 [info ] [MainThread]: Finished running 1 table model, 2 view models in 0 hours 0 minutes and 8.55 seconds (8.55s).
[0m13:55:15.279816 [debug] [MainThread]: Command end result
[0m13:55:15.302651 [info ] [MainThread]: 
[0m13:55:15.306622 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:55:15.308612 [info ] [MainThread]: 
[0m13:55:15.310606 [error] [MainThread]: [33mRuntime Error in model xcustomers (models\example\xcustomers.sql)[0m
[0m13:55:15.312601 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m13:55:15.314599 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m13:55:15.317587 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 13 pos 9
[0m13:55:15.324383 [info ] [MainThread]: 
[0m13:55:15.327375 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m13:55:15.329370 [debug] [MainThread]: Command `dbt run` failed at 13:55:15.329370 after 11.29 seconds
[0m13:55:15.330366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDA424CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BDC5C9F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BD7968E50>]}
[0m13:55:15.331363 [debug] [MainThread]: Flushing usage events
[0m13:56:07.911654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6601007D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6600F0510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6601D2510>]}


============================== 13:56:07.918635 | b0cad045-efdf-4b1a-a139-fef977c615e6 ==============================
[0m13:56:07.918635 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:56:07.921629 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:56:10.512374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D66052E390>]}
[0m13:56:10.537280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D67183E490>]}
[0m13:56:10.568197 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:56:10.775647 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:56:10.776645 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\example\xcustomers.sql
[0m13:56:10.815537 [debug] [MainThread]: 1699: static parser successfully parsed example\xcustomers.sql
[0m13:56:10.866404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6718BBD90>]}
[0m13:56:10.888347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6718F0CD0>]}
[0m13:56:10.889341 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:56:10.890341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6717CC790>]}
[0m13:56:10.893333 [info ] [MainThread]: 
[0m13:56:10.896323 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:56:10.900313 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:56:10.925247 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:56:10.926244 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:56:10.928239 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:56:11.533630 [debug] [ThreadPool]: SQL status: OK in 0.6100000143051147 seconds
[0m13:56:11.542606 [debug] [ThreadPool]: On list_schemas: Close
[0m13:56:11.723125 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw_test_rw'
[0m13:56:11.736092 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:56:11.737094 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m13:56:11.738087 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show tables in `test_rw_test_rw`
  
[0m13:56:11.739084 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:56:12.320568 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m13:56:12.332535 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m13:56:12.333532 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show views in `test_rw_test_rw`
  
[0m13:56:12.713521 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m13:56:12.717510 [debug] [ThreadPool]: On list_None_test_rw_test_rw: ROLLBACK
[0m13:56:12.718507 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:56:12.719505 [debug] [ThreadPool]: On list_None_test_rw_test_rw: Close
[0m13:56:12.901022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D671A42B50>]}
[0m13:56:12.902020 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:56:12.903023 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:56:12.905052 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:56:12.907008 [info ] [MainThread]: 
[0m13:56:12.914992 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:56:12.916980 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw_test_rw.my_first_dbt_model ................ [RUN]
[0m13:56:12.920970 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.my_first_dbt_model'
[0m13:56:12.922965 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:56:12.926953 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:56:12.928948 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:56:12.923962 => 13:56:12.928948
[0m13:56:12.929946 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:56:12.955888 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:56:12.957874 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:56:12.958870 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw_test_rw`.`my_first_dbt_model`
  
[0m13:56:12.959867 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:56:13.575233 [debug] [Thread-1 (]: SQL status: OK in 0.6200000047683716 seconds
[0m13:56:13.650030 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:56:13.654019 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:56:13.655017 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw_test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:56:15.870112 [debug] [Thread-1 (]: SQL status: OK in 2.2100000381469727 seconds
[0m13:56:15.924964 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:56:12.930942 => 13:56:15.923968
[0m13:56:15.925962 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:56:15.926988 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:56:15.927957 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:56:16.088568 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D671A21C50>]}
[0m13:56:16.089559 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw_test_rw.my_first_dbt_model ........... [[32mOK[0m in 3.17s]
[0m13:56:16.091520 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:56:16.092518 [debug] [Thread-1 (]: Began running node model.jaffle_shop.xcustomers
[0m13:56:16.094513 [info ] [Thread-1 (]: 2 of 3 START sql view model test_rw_test_rw.xcustomers ......................... [RUN]
[0m13:56:16.096508 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.xcustomers)
[0m13:56:16.097539 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.xcustomers
[0m13:56:16.101494 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.xcustomers"
[0m13:56:16.106482 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.xcustomers (compile): 13:56:16.098539 => 13:56:16.106482
[0m13:56:16.107480 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.xcustomers
[0m13:56:16.135431 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.xcustomers"
[0m13:56:16.140390 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:56:16.142386 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.xcustomers"
[0m13:56:16.143390 [debug] [Thread-1 (]: On model.jaffle_shop.xcustomers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.xcustomers"} */
create or replace view `test_rw_test_rw`.`xcustomers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:56:16.144383 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:56:16.773734 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.xcustomers"} */
create or replace view `test_rw_test_rw`.`xcustomers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:56:16.774732 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 25 pos 9
[0m13:56:16.775729 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 25 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:609)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:501)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:361)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:339)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:324)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:373)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 25 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:586)
	... 20 more

[0m13:56:16.776695 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x01\xed\xf5{`\\\x15"\x8a\xe7\xa2\xe4\xf3N7G'
[0m13:56:16.778689 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.xcustomers (execute): 13:56:16.108477 => 13:56:16.777724
[0m13:56:16.778689 [debug] [Thread-1 (]: On model.jaffle_shop.xcustomers: ROLLBACK
[0m13:56:16.779721 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:56:16.780684 [debug] [Thread-1 (]: On model.jaffle_shop.xcustomers: Close
[0m13:56:16.948237 [debug] [Thread-1 (]: Runtime Error in model xcustomers (models\example\xcustomers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 25 pos 9
[0m13:56:16.949236 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D671A23010>]}
[0m13:56:16.950233 [error] [Thread-1 (]: 2 of 3 ERROR creating sql view model test_rw_test_rw.xcustomers ................ [[31mERROR[0m in 0.85s]
[0m13:56:16.952228 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.xcustomers
[0m13:56:16.954244 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:56:16.955218 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw_test_rw.my_second_dbt_model ................ [RUN]
[0m13:56:16.957213 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.xcustomers, now model.jaffle_shop.my_second_dbt_model)
[0m13:56:16.958210 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:56:16.967185 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:56:16.970179 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:56:16.959207 => 13:56:16.969181
[0m13:56:16.971177 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:56:16.977160 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:56:16.982147 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:56:16.984141 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:56:16.985138 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw_test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw_test_rw`.`my_first_dbt_model`
where id = 1

[0m13:56:16.986138 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:56:18.491123 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m13:56:18.495112 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:56:16.971177 => 13:56:18.494115
[0m13:56:18.495112 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:56:18.496110 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:56:18.497106 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:56:18.661669 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0cad045-efdf-4b1a-a139-fef977c615e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D671918C90>]}
[0m13:56:18.662666 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw_test_rw.my_second_dbt_model ........... [[32mOK[0m in 1.71s]
[0m13:56:18.664661 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:56:18.667655 [debug] [MainThread]: On master: ROLLBACK
[0m13:56:18.670647 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:56:18.889063 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:56:18.890061 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:56:18.891058 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:56:18.891058 [debug] [MainThread]: On master: ROLLBACK
[0m13:56:18.892055 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:56:18.893051 [debug] [MainThread]: On master: Close
[0m13:56:19.084540 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:56:19.085538 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:56:19.087533 [debug] [MainThread]: Connection 'list_None_test_rw_test_rw' was properly closed.
[0m13:56:19.087533 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:56:19.089527 [info ] [MainThread]: 
[0m13:56:19.090524 [info ] [MainThread]: Finished running 1 table model, 2 view models in 0 hours 0 minutes and 8.19 seconds (8.19s).
[0m13:56:19.093517 [debug] [MainThread]: Command end result
[0m13:56:19.115460 [info ] [MainThread]: 
[0m13:56:19.116455 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:56:19.118453 [info ] [MainThread]: 
[0m13:56:19.120446 [error] [MainThread]: [33mRuntime Error in model xcustomers (models\example\xcustomers.sql)[0m
[0m13:56:19.122440 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m13:56:19.124435 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m13:56:19.126430 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 25 pos 9
[0m13:56:19.127434 [info ] [MainThread]: 
[0m13:56:19.128426 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m13:56:19.131416 [debug] [MainThread]: Command `dbt run` failed at 13:56:19.131416 after 11.32 seconds
[0m13:56:19.134417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6601CBB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D65FE04B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D65FC60F10>]}
[0m13:56:19.136403 [debug] [MainThread]: Flushing usage events
[0m13:58:12.049398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BDC27010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BDF857D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB2DCD90>]}


============================== 13:58:12.059371 | 1ac41901-09f3-46dc-a758-245f8dbe4fc5 ==============================
[0m13:58:12.059371 [info ] [MainThread]: Running with dbt=1.5.0
[0m13:58:12.061367 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:58:15.580984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF6521D0>]}
[0m13:58:15.618882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF66EA50>]}
[0m13:58:15.662765 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m13:58:15.965959 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m13:58:15.967951 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\example\customers.sql
[0m13:58:15.969945 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models\example\xcustomers.sql
[0m13:58:16.035770 [debug] [MainThread]: 1699: static parser successfully parsed example\customers.sql
[0m13:58:16.115558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF5E5B90>]}
[0m13:58:16.151462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF71D590>]}
[0m13:58:16.153458 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m13:58:16.157454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF6EB910>]}
[0m13:58:16.164428 [info ] [MainThread]: 
[0m13:58:16.168417 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:58:16.178391 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m13:58:16.218284 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m13:58:16.219281 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:58:16.221276 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:17.045080 [debug] [ThreadPool]: SQL status: OK in 0.8199999928474426 seconds
[0m13:58:17.062034 [debug] [ThreadPool]: On list_schemas: Close
[0m13:58:17.256517 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw_test_rw'
[0m13:58:17.291423 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:17.293417 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m13:58:17.294415 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show tables in `test_rw_test_rw`
  
[0m13:58:17.296409 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:17.953658 [debug] [ThreadPool]: SQL status: OK in 0.6600000262260437 seconds
[0m13:58:17.978591 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m13:58:17.980585 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show views in `test_rw_test_rw`
  
[0m13:58:18.331649 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m13:58:18.341623 [debug] [ThreadPool]: On list_None_test_rw_test_rw: ROLLBACK
[0m13:58:18.343619 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:58:18.344615 [debug] [ThreadPool]: On list_None_test_rw_test_rw: Close
[0m13:58:18.544083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF6EAF10>]}
[0m13:58:18.546078 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:18.547076 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:58:18.549070 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:58:18.552063 [info ] [MainThread]: 
[0m13:58:18.577993 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m13:58:18.579989 [info ] [Thread-1 (]: 1 of 3 START sql view model test_rw_test_rw.customers .......................... [RUN]
[0m13:58:18.584975 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m13:58:18.585972 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m13:58:18.594949 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m13:58:18.600935 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 13:58:18.586969 => 13:58:18.599934
[0m13:58:18.602927 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m13:58:18.817355 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m13:58:18.823339 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:18.825333 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m13:58:18.827328 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
create or replace view `test_rw_test_rw`.`customers`
  
  
  as
    with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from test_rw.jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

[0m13:58:18.830320 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:58:20.108993 [debug] [Thread-1 (]: SQL status: OK in 1.2799999713897705 seconds
[0m13:58:20.146893 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 13:58:18.603924 => 13:58:20.145895
[0m13:58:20.148888 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m13:58:20.150882 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:58:20.152877 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m13:58:20.336388 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF748990>]}
[0m13:58:20.339380 [info ] [Thread-1 (]: 1 of 3 OK created sql view model test_rw_test_rw.customers ..................... [[32mOK[0m in 1.75s]
[0m13:58:20.343369 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m13:58:20.346361 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m13:58:20.347358 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw_test_rw.my_first_dbt_model ................ [RUN]
[0m13:58:20.351347 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now model.jaffle_shop.my_first_dbt_model)
[0m13:58:20.353342 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m13:58:20.365310 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m13:58:20.370303 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 13:58:20.354340 => 13:58:20.369300
[0m13:58:20.373290 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m13:58:20.436124 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:20.438117 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:58:20.441108 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw_test_rw`.`my_first_dbt_model`
  
[0m13:58:20.443103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:58:21.112320 [debug] [Thread-1 (]: SQL status: OK in 0.6700000166893005 seconds
[0m13:58:21.226016 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m13:58:21.230005 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m13:58:21.231002 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw_test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:58:23.296495 [debug] [Thread-1 (]: SQL status: OK in 2.059999942779541 seconds
[0m13:58:23.336389 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 13:58:20.374287 => 13:58:23.335392
[0m13:58:23.337386 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m13:58:23.338383 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:58:23.340378 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m13:58:23.517905 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF69F990>]}
[0m13:58:23.519899 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw_test_rw.my_first_dbt_model ........... [[32mOK[0m in 3.17s]
[0m13:58:23.521894 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m13:58:23.523889 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m13:58:23.525885 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw_test_rw.my_second_dbt_model ................ [RUN]
[0m13:58:23.528878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m13:58:23.529874 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m13:58:23.537852 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m13:58:23.541841 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 13:58:23.530870 => 13:58:23.540844
[0m13:58:23.542838 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m13:58:23.549819 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m13:58:23.553809 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:23.556821 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m13:58:23.560791 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw_test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw_test_rw`.`my_first_dbt_model`
where id = 1

[0m13:58:23.561789 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:58:24.897229 [debug] [Thread-1 (]: SQL status: OK in 1.340000033378601 seconds
[0m13:58:24.904239 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 13:58:23.543836 => 13:58:24.903211
[0m13:58:24.905213 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m13:58:24.907201 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:58:24.908229 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m13:58:25.073759 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ac41901-09f3-46dc-a758-245f8dbe4fc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271CF894C50>]}
[0m13:58:25.075782 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw_test_rw.my_second_dbt_model ........... [[32mOK[0m in 1.55s]
[0m13:58:25.078746 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m13:58:25.082734 [debug] [MainThread]: On master: ROLLBACK
[0m13:58:25.084727 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:58:25.317109 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:58:25.318108 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:25.319103 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:58:25.320099 [debug] [MainThread]: On master: ROLLBACK
[0m13:58:25.321097 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:58:25.323094 [debug] [MainThread]: On master: Close
[0m13:58:25.490645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:58:25.491643 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m13:58:25.492640 [debug] [MainThread]: Connection 'list_None_test_rw_test_rw' was properly closed.
[0m13:58:25.493637 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m13:58:25.496630 [info ] [MainThread]: 
[0m13:58:25.498623 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 9.33 seconds (9.33s).
[0m13:58:25.502614 [debug] [MainThread]: Command end result
[0m13:58:25.540283 [info ] [MainThread]: 
[0m13:58:25.545500 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:58:25.547494 [info ] [MainThread]: 
[0m13:58:25.549509 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m13:58:25.553477 [debug] [MainThread]: Command `dbt run` succeeded at 13:58:25.553477 after 13.62 seconds
[0m13:58:25.555472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BDF87E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BD95E350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271B903F450>]}
[0m13:58:25.561459 [debug] [MainThread]: Flushing usage events
[0m14:04:18.277895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D99E958CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D99E77CE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D99E99BB90>]}


============================== 14:04:18.284481 | 5237651a-ab7d-4ec9-845b-fd9b53b2b276 ==============================
[0m14:04:18.284481 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:04:18.286477 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:04:20.599397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D99E97E390>]}
[0m14:04:20.630316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B03DF450>]}
[0m14:04:20.662228 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:04:20.843329 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:04:20.844324 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\example\customers.sql
[0m14:04:20.881193 [debug] [MainThread]: 1699: static parser successfully parsed example\customers.sql
[0m14:04:20.936046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B045FA50>]}
[0m14:04:20.957988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B0492F10>]}
[0m14:04:20.958983 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:04:20.960979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B0317610>]}
[0m14:04:20.963971 [info ] [MainThread]: 
[0m14:04:20.965966 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:04:20.970951 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:04:21.008851 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:04:21.009849 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:04:21.010846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:04:21.623819 [debug] [ThreadPool]: SQL status: OK in 0.6100000143051147 seconds
[0m14:04:21.634789 [debug] [ThreadPool]: On list_schemas: Close
[0m14:04:21.797355 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw_test_rw'
[0m14:04:21.814313 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:04:21.816306 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m14:04:21.817302 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show tables in `test_rw_test_rw`
  
[0m14:04:21.818300 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:04:22.413954 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m14:04:22.430679 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_test_rw"
[0m14:04:22.433134 [debug] [ThreadPool]: On list_None_test_rw_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_test_rw"} */
show views in `test_rw_test_rw`
  
[0m14:04:22.941135 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m14:04:22.946121 [debug] [ThreadPool]: On list_None_test_rw_test_rw: ROLLBACK
[0m14:04:22.947120 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:04:22.948116 [debug] [ThreadPool]: On list_None_test_rw_test_rw: Close
[0m14:04:23.104510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B05E2B50>]}
[0m14:04:23.105543 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:04:23.106539 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:04:23.107542 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:04:23.109495 [info ] [MainThread]: 
[0m14:04:23.117474 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:04:23.118507 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw_test_rw.customers ......................... [RUN]
[0m14:04:23.122460 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m14:04:23.124458 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:04:23.135427 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:04:23.139415 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:04:23.127448 => 14:04:23.138417
[0m14:04:23.140412 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:04:23.169364 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:04:23.170367 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:04:23.171365 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

      describe extended `test_rw_test_rw`.`customers`
  
[0m14:04:23.172362 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:04:23.901408 [debug] [Thread-1 (]: SQL status: OK in 0.7300000190734863 seconds
[0m14:04:23.920329 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:04:23.921326 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
drop view if exists `test_rw_test_rw`.`customers`
[0m14:04:24.952095 [debug] [Thread-1 (]: SQL status: OK in 1.0299999713897705 seconds
[0m14:04:25.014934 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m14:04:25.016888 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:04:25.017885 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

  
    
        create or replace table `test_rw_test_rw`.`customers`
      
      
    using delta
      
      
      
      
      
      
      as
      

with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from test_rw.jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
  
[0m14:04:33.692143 [debug] [Thread-1 (]: SQL status: OK in 8.670000076293945 seconds
[0m14:04:33.948757 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:04:23.141415 => 14:04:33.948757
[0m14:04:33.949754 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m14:04:33.950751 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:04:33.951748 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m14:04:34.160580 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B05CD650>]}
[0m14:04:34.161577 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw_test_rw.customers .................... [[32mOK[0m in 11.04s]
[0m14:04:34.164573 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:04:34.165570 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m14:04:34.166573 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw_test_rw.my_first_dbt_model ................ [RUN]
[0m14:04:34.168589 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now model.jaffle_shop.my_first_dbt_model)
[0m14:04:34.169559 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m14:04:34.173584 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m14:04:34.177535 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 14:04:34.170583 => 14:04:34.177535
[0m14:04:34.178532 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m14:04:34.189508 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:04:34.190500 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m14:04:34.191497 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw_test_rw`.`my_first_dbt_model`
  
[0m14:04:34.192497 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:04:34.756282 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m14:04:34.762797 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m14:04:34.765751 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m14:04:34.766747 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw_test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:04:37.019482 [debug] [Thread-1 (]: SQL status: OK in 2.25 seconds
[0m14:04:37.023469 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 14:04:34.179529 => 14:04:37.023469
[0m14:04:37.024466 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m14:04:37.025464 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:04:37.026423 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m14:04:37.184001 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B063F290>]}
[0m14:04:37.184999 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw_test_rw.my_first_dbt_model ........... [[32mOK[0m in 3.02s]
[0m14:04:37.186994 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m14:04:37.187991 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m14:04:37.188989 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw_test_rw.my_second_dbt_model ................ [RUN]
[0m14:04:37.190985 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m14:04:37.191981 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m14:04:37.200958 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m14:04:37.204947 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 14:04:37.192978 => 14:04:37.204947
[0m14:04:37.206941 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m14:04:37.265785 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m14:04:37.271769 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:04:37.272767 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m14:04:37.273765 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw_test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw_test_rw`.`my_first_dbt_model`
where id = 1

[0m14:04:37.274761 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:04:38.860528 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m14:04:38.865542 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 14:04:37.207948 => 14:04:38.865542
[0m14:04:38.866510 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m14:04:38.868536 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:04:38.869501 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m14:04:39.032068 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5237651a-ab7d-4ec9-845b-fd9b53b2b276', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9B06272D0>]}
[0m14:04:39.034063 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw_test_rw.my_second_dbt_model ........... [[32mOK[0m in 1.84s]
[0m14:04:39.037056 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m14:04:39.040046 [debug] [MainThread]: On master: ROLLBACK
[0m14:04:39.041043 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:04:39.298357 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:04:39.299354 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:04:39.300351 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:04:39.302346 [debug] [MainThread]: On master: ROLLBACK
[0m14:04:39.303342 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:04:39.304340 [debug] [MainThread]: On master: Close
[0m14:04:39.463915 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:04:39.464913 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:04:39.465909 [debug] [MainThread]: Connection 'list_None_test_rw_test_rw' was properly closed.
[0m14:04:39.466906 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m14:04:39.468901 [info ] [MainThread]: 
[0m14:04:39.470896 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 18.50 seconds (18.50s).
[0m14:04:39.474886 [debug] [MainThread]: Command end result
[0m14:04:39.505803 [info ] [MainThread]: 
[0m14:04:39.507798 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:04:39.510789 [info ] [MainThread]: 
[0m14:04:39.512785 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:04:39.520763 [debug] [MainThread]: Command `dbt run` succeeded at 14:04:39.519773 after 21.33 seconds
[0m14:04:39.521761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D99F0DE8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D99E727D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D99EC91710>]}
[0m14:04:39.523755 [debug] [MainThread]: Flushing usage events
[0m14:05:49.504932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163820F9DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016381EDB850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016382335E10>]}


============================== 14:05:49.512878 | 9a4475c9-4400-4359-a42d-318d6888949e ==============================
[0m14:05:49.512878 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:05:49.515868 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:05:52.257026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163937821D0>]}
[0m14:05:52.281924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016393775910>]}
[0m14:05:52.313873 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:05:52.333821 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m14:05:52.334786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163938C6450>]}
[0m14:05:54.891717 [debug] [MainThread]: 1699: static parser successfully parsed example\customers.sql
[0m14:05:54.914649 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m14:05:54.922625 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m14:05:55.061254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639383E4D0>]}
[0m14:05:55.079236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163937A94D0>]}
[0m14:05:55.080233 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:05:55.082230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639383D710>]}
[0m14:05:55.085220 [info ] [MainThread]: 
[0m14:05:55.087217 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:05:55.091205 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:05:55.110238 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:05:55.111235 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:05:55.111235 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:05:55.685814 [debug] [ThreadPool]: SQL status: OK in 0.5699999928474426 seconds
[0m14:05:55.693828 [debug] [ThreadPool]: On list_schemas: Close
[0m14:05:55.869452 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:05:55.884447 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:05:55.885446 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:05:55.885446 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:05:55.886408 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:05:56.424149 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m14:05:56.435390 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:05:56.436422 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:05:56.852448 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m14:05:56.858432 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:05:56.858432 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:05:56.859463 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:05:57.020107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016381295450>]}
[0m14:05:57.022109 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:05:57.023096 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:05:57.024128 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:05:57.026097 [info ] [MainThread]: 
[0m14:05:57.034069 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:05:57.035065 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw.customers ................................. [RUN]
[0m14:05:57.037061 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m14:05:57.038093 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:05:57.042048 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:05:57.046070 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:05:57.039088 => 14:05:57.045039
[0m14:05:57.046070 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:05:57.143774 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m14:05:57.146766 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:05:57.147764 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:05:57.149767 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

  
    
        create or replace table `test_rw`.`customers`
      
      
    using delta
      
      
      
      
      
      
      as
      

with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from test_rw.jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
  
[0m14:05:57.151753 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:06:00.262845 [debug] [Thread-1 (]: SQL status: OK in 3.109999895095825 seconds
[0m14:06:00.290771 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:05:57.047069 => 14:06:00.289774
[0m14:06:00.290771 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m14:06:00.291731 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:06:00.291731 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m14:06:00.449962 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016393742910>]}
[0m14:06:00.450986 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw.customers ............................ [[32mOK[0m in 3.41s]
[0m14:06:00.451959 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:06:00.452961 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m14:06:00.453957 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw.my_first_dbt_model ........................ [RUN]
[0m14:06:00.455952 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now model.jaffle_shop.my_first_dbt_model)
[0m14:06:00.456984 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m14:06:00.461936 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m14:06:00.465736 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 14:06:00.456984 => 14:06:00.464927
[0m14:06:00.467919 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m14:06:00.478889 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:06:00.478889 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m14:06:00.479887 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw`.`my_first_dbt_model`
  
[0m14:06:00.479887 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:06:01.053021 [debug] [Thread-1 (]: SQL status: OK in 0.5699999928474426 seconds
[0m14:06:01.060999 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m14:06:01.062994 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m14:06:01.063991 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:06:03.136567 [debug] [Thread-1 (]: SQL status: OK in 2.069999933242798 seconds
[0m14:06:03.142561 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 14:06:00.468923 => 14:06:03.142561
[0m14:06:03.143556 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m14:06:03.144516 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:06:03.144516 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m14:06:03.319676 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016393988F50>]}
[0m14:06:03.320673 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw.my_first_dbt_model ................... [[32mOK[0m in 2.86s]
[0m14:06:03.322004 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m14:06:03.323002 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m14:06:03.323999 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw.my_second_dbt_model ........................ [RUN]
[0m14:06:03.325994 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m14:06:03.325994 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m14:06:03.330979 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m14:06:03.335969 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 14:06:03.326990 => 14:06:03.335969
[0m14:06:03.336963 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m14:06:03.361935 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m14:06:03.364889 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:06:03.367889 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m14:06:03.369876 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw`.`my_first_dbt_model`
where id = 1

[0m14:06:03.371871 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:06:04.932430 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m14:06:04.935420 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 14:06:03.337961 => 14:06:04.934423
[0m14:06:04.935420 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m14:06:04.936417 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:06:04.936417 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m14:06:05.098047 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a4475c9-4400-4359-a42d-318d6888949e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163939FB410>]}
[0m14:06:05.099050 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw.my_second_dbt_model ................... [[32mOK[0m in 1.77s]
[0m14:06:05.101043 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m14:06:05.103042 [debug] [MainThread]: On master: ROLLBACK
[0m14:06:05.104035 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:06:05.305480 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:06:05.306480 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:06:05.306480 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:06:05.307476 [debug] [MainThread]: On master: ROLLBACK
[0m14:06:05.307476 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:06:05.308474 [debug] [MainThread]: On master: Close
[0m14:06:05.466636 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:06:05.467673 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:06:05.468667 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:06:05.468667 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m14:06:05.470657 [info ] [MainThread]: 
[0m14:06:05.471622 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 10.38 seconds (10.38s).
[0m14:06:05.473615 [debug] [MainThread]: Command end result
[0m14:06:05.492564 [info ] [MainThread]: 
[0m14:06:05.493797 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:06:05.494862 [info ] [MainThread]: 
[0m14:06:05.495909 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:06:05.498064 [debug] [MainThread]: Command `dbt run` succeeded at 14:06:05.497029 after 16.09 seconds
[0m14:06:05.499028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016381CB0B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016381E25BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163FD12F450>]}
[0m14:06:05.501020 [debug] [MainThread]: Flushing usage events
[0m14:09:06.393707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5656395D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5678F3A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56803FAD0>]}


============================== 14:09:06.397730 | 7e8176dd-854e-43a3-beee-27a4c2f54a95 ==============================
[0m14:09:06.397730 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:09:06.398693 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:09:08.121598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F565270550>]}
[0m14:09:08.143540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F579327550>]}
[0m14:09:08.170511 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:09:08.187451 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m14:09:08.189419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5794767D0>]}
[0m14:09:09.606629 [debug] [MainThread]: 1699: static parser successfully parsed example\customers.sql
[0m14:09:09.625579 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m14:09:09.632561 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m14:09:09.769195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57950C950>]}
[0m14:09:09.787183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5792DD310>]}
[0m14:09:09.788178 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:09:09.789143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57942B950>]}
[0m14:09:09.791138 [info ] [MainThread]: 
[0m14:09:09.793130 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:09:09.798120 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:09:09.815073 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:09:09.816071 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:09:09.816071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:09:10.448718 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m14:09:10.455665 [debug] [ThreadPool]: On list_schemas: Close
[0m14:09:10.631813 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test_rw_badger)
[0m14:09:10.632812 [debug] [ThreadPool]: Creating schema "schema: "test_rw_badger"
"
[0m14:09:10.638796 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:10.639793 [debug] [ThreadPool]: Using databricks connection "create__test_rw_badger"
[0m14:09:10.640791 [debug] [ThreadPool]: On create__test_rw_badger: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "create__test_rw_badger"} */
create schema if not exists `test_rw_badger`
  
[0m14:09:10.640791 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:09:11.299614 [debug] [ThreadPool]: SQL status: OK in 0.6600000262260437 seconds
[0m14:09:11.301614 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:09:11.302613 [debug] [ThreadPool]: On create__test_rw_badger: ROLLBACK
[0m14:09:11.302613 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:09:11.303610 [debug] [ThreadPool]: On create__test_rw_badger: Close
[0m14:09:11.466367 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw_badger'
[0m14:09:11.477341 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:11.478338 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw_badger"
[0m14:09:11.478338 [debug] [ThreadPool]: On list_None_test_rw_badger: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw_badger"} */
show tables in `test_rw_badger`
  
[0m14:09:11.479336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:09:12.016731 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m14:09:12.020725 [debug] [ThreadPool]: On list_None_test_rw_badger: ROLLBACK
[0m14:09:12.021729 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:09:12.021729 [debug] [ThreadPool]: On list_None_test_rw_badger: Close
[0m14:09:12.185710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5656CDE90>]}
[0m14:09:12.185710 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:12.186739 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:09:12.187742 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:09:12.188699 [info ] [MainThread]: 
[0m14:09:12.195682 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:09:12.198674 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw_badger.customers .......................... [RUN]
[0m14:09:12.202671 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m14:09:12.204657 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:09:12.208646 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:09:12.210640 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:09:12.204657 => 14:09:12.209643
[0m14:09:12.210640 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:09:12.294450 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m14:09:12.297408 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:12.297408 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:09:12.298440 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

  
    
        create or replace table `test_rw_badger`.`customers`
      
      
    using delta
      
      
      
      
      
      
      as
      

with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from test_rw.jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
  
[0m14:09:12.299404 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:09:15.113957 [debug] [Thread-1 (]: SQL status: OK in 2.809999942779541 seconds
[0m14:09:15.148821 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:09:12.211639 => 14:09:15.148821
[0m14:09:15.149819 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m14:09:15.149819 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:09:15.150816 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m14:09:15.336801 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5792D2C90>]}
[0m14:09:15.337834 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw_badger.customers ..................... [[32mOK[0m in 3.14s]
[0m14:09:15.339794 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:09:15.340789 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m14:09:15.341821 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw_badger.my_first_dbt_model ................. [RUN]
[0m14:09:15.342785 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now model.jaffle_shop.my_first_dbt_model)
[0m14:09:15.343816 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m14:09:15.348772 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m14:09:15.353756 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 14:09:15.344812 => 14:09:15.352775
[0m14:09:15.354753 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m14:09:15.362765 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m14:09:15.364725 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:15.365722 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m14:09:15.366719 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw_badger`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:09:15.366719 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:09:17.504565 [debug] [Thread-1 (]: SQL status: OK in 2.140000104904175 seconds
[0m14:09:17.508555 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 14:09:15.355756 => 14:09:17.508555
[0m14:09:17.509518 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m14:09:17.509518 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:09:17.510548 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m14:09:17.691069 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5793B3350>]}
[0m14:09:17.692064 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw_badger.my_first_dbt_model ............ [[32mOK[0m in 2.35s]
[0m14:09:17.694026 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m14:09:17.696018 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m14:09:17.697018 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw_badger.my_second_dbt_model ................. [RUN]
[0m14:09:17.699011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m14:09:17.700042 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m14:09:17.708988 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m14:09:17.711975 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 14:09:17.700042 => 14:09:17.711975
[0m14:09:17.712973 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m14:09:17.745914 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m14:09:17.747890 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:17.748877 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m14:09:17.748877 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw_badger`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw_badger`.`my_first_dbt_model`
where id = 1

[0m14:09:17.749878 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:09:18.879923 [debug] [Thread-1 (]: SQL status: OK in 1.1299999952316284 seconds
[0m14:09:18.882916 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 14:09:17.712973 => 14:09:18.881919
[0m14:09:18.882916 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m14:09:18.883926 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:09:18.883926 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m14:09:19.042904 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e8176dd-854e-43a3-beee-27a4c2f54a95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F579590190>]}
[0m14:09:19.043939 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw_badger.my_second_dbt_model ............ [[32mOK[0m in 1.34s]
[0m14:09:19.045898 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m14:09:19.047890 [debug] [MainThread]: On master: ROLLBACK
[0m14:09:19.048928 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:09:19.249838 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:09:19.250838 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:19.250838 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:09:19.251834 [debug] [MainThread]: On master: ROLLBACK
[0m14:09:19.251834 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:09:19.252832 [debug] [MainThread]: On master: Close
[0m14:09:19.413114 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:09:19.414113 [debug] [MainThread]: Connection 'create__test_rw_badger' was properly closed.
[0m14:09:19.414113 [debug] [MainThread]: Connection 'list_None_test_rw_badger' was properly closed.
[0m14:09:19.415110 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m14:09:19.416107 [info ] [MainThread]: 
[0m14:09:19.417132 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 9.62 seconds (9.62s).
[0m14:09:19.420127 [debug] [MainThread]: Command end result
[0m14:09:19.440075 [info ] [MainThread]: 
[0m14:09:19.442071 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:09:19.444064 [info ] [MainThread]: 
[0m14:09:19.445063 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:09:19.447088 [debug] [MainThread]: Command `dbt run` succeeded at 14:09:19.446057 after 13.11 seconds
[0m14:09:19.447088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F567C60A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F562DCF490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F562DCF450>]}
[0m14:09:19.448090 [debug] [MainThread]: Flushing usage events
[0m14:15:00.127456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C668D41490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C668D40CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C668D08AD0>]}


============================== 14:15:00.140423 | 23253aca-c0fd-40ca-afab-63523378b37e ==============================
[0m14:15:00.140423 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:15:00.152401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:15:03.171865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C66949CB50>]}
[0m14:15:03.200787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A78DB90>]}
[0m14:15:03.235697 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:15:03.255670 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m14:15:03.257638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A8B6510>]}
[0m14:15:05.449769 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m14:15:05.475730 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m14:15:05.483678 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m14:15:05.658217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6694B9550>]}
[0m14:15:05.680154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A822E10>]}
[0m14:15:05.682149 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:15:05.683145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A861810>]}
[0m14:15:05.687136 [info ] [MainThread]: 
[0m14:15:05.690128 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:15:05.695113 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:15:05.721044 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:15:05.722042 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:15:05.729041 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:06.570771 [debug] [ThreadPool]: SQL status: OK in 0.8399999737739563 seconds
[0m14:15:06.584735 [debug] [ThreadPool]: On list_schemas: Close
[0m14:15:06.747332 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:15:06.766247 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:06.767245 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:15:06.768242 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:15:06.769239 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:07.347694 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m14:15:07.364646 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:15:07.365644 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:15:07.703739 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m14:15:07.711719 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:15:07.713712 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:15:07.714710 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:15:07.884255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A88D950>]}
[0m14:15:07.885253 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:07.886251 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:15:07.887248 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:15:07.889243 [info ] [MainThread]: 
[0m14:15:07.900216 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:15:07.902208 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw.customers ................................. [RUN]
[0m14:15:07.909196 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m14:15:07.911186 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:15:07.916171 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:15:07.920161 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:15:07.912182 => 14:15:07.920161
[0m14:15:07.921157 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:15:07.955069 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:07.956066 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:15:07.960067 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

      describe extended `test_rw`.`customers`
  
[0m14:15:07.963047 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:15:08.610530 [debug] [Thread-1 (]: SQL status: OK in 0.6499999761581421 seconds
[0m14:15:08.719241 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m14:15:08.725224 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:15:08.726221 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

  
    
        create or replace table `test_rw`.`customers`
      
      
    using delta
      
      
      
      
      
      
      as
      with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from test_rw.jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
  
[0m14:15:11.511907 [debug] [Thread-1 (]: SQL status: OK in 2.7799999713897705 seconds
[0m14:15:11.545816 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:15:07.922156 => 14:15:11.545816
[0m14:15:11.546813 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m14:15:11.546813 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:15:11.547810 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m14:15:11.717533 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A7CA550>]}
[0m14:15:11.718526 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw.customers ............................ [[32mOK[0m in 3.81s]
[0m14:15:11.720529 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:15:11.720529 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_first_dbt_model
[0m14:15:11.721564 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw.my_first_dbt_model ........................ [RUN]
[0m14:15:11.723521 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now model.jaffle_shop.my_first_dbt_model)
[0m14:15:11.724518 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_first_dbt_model
[0m14:15:11.728508 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_first_dbt_model"
[0m14:15:11.732498 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (compile): 14:15:11.725514 => 14:15:11.732498
[0m14:15:11.733527 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_first_dbt_model
[0m14:15:11.738513 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:11.739513 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m14:15:11.740475 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

      describe extended `test_rw`.`my_first_dbt_model`
  
[0m14:15:11.742469 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:12.306373 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m14:15:12.322330 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_first_dbt_model"
[0m14:15:12.325322 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_first_dbt_model"
[0m14:15:12.327317 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_first_dbt_model"} */

  
    
        create or replace table `test_rw`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:15:14.267327 [debug] [Thread-1 (]: SQL status: OK in 1.940000057220459 seconds
[0m14:15:14.272314 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_first_dbt_model (execute): 14:15:11.734528 => 14:15:14.272314
[0m14:15:14.273310 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: ROLLBACK
[0m14:15:14.274341 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:15:14.274341 [debug] [Thread-1 (]: On model.jaffle_shop.my_first_dbt_model: Close
[0m14:15:14.441904 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A766110>]}
[0m14:15:14.442904 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw.my_first_dbt_model ................... [[32mOK[0m in 2.72s]
[0m14:15:14.444851 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_first_dbt_model
[0m14:15:14.445850 [debug] [Thread-1 (]: Began running node model.jaffle_shop.my_second_dbt_model
[0m14:15:14.446847 [info ] [Thread-1 (]: 3 of 3 START sql view model test_rw.my_second_dbt_model ........................ [RUN]
[0m14:15:14.448843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.my_first_dbt_model, now model.jaffle_shop.my_second_dbt_model)
[0m14:15:14.449840 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.my_second_dbt_model
[0m14:15:14.458820 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.my_second_dbt_model"
[0m14:15:14.461806 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (compile): 14:15:14.450837 => 14:15:14.460808
[0m14:15:14.462803 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.my_second_dbt_model
[0m14:15:14.491729 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.my_second_dbt_model"
[0m14:15:14.495716 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:14.497712 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.my_second_dbt_model"
[0m14:15:14.498710 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.my_second_dbt_model"} */
create or replace view `test_rw`.`my_second_dbt_model`
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `test_rw`.`my_first_dbt_model`
where id = 1

[0m14:15:14.499707 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:15.882594 [debug] [Thread-1 (]: SQL status: OK in 1.3799999952316284 seconds
[0m14:15:15.885586 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.my_second_dbt_model (execute): 14:15:14.463803 => 14:15:15.885586
[0m14:15:15.886583 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: ROLLBACK
[0m14:15:15.886583 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:15:15.887581 [debug] [Thread-1 (]: On model.jaffle_shop.my_second_dbt_model: Close
[0m14:15:16.048151 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23253aca-c0fd-40ca-afab-63523378b37e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C67A766110>]}
[0m14:15:16.049185 [info ] [Thread-1 (]: 3 of 3 OK created sql view model test_rw.my_second_dbt_model ................... [[32mOK[0m in 1.60s]
[0m14:15:16.051143 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.my_second_dbt_model
[0m14:15:16.053171 [debug] [MainThread]: On master: ROLLBACK
[0m14:15:16.053171 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:15:16.261652 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:15:16.262683 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:16.262683 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:15:16.263650 [debug] [MainThread]: On master: ROLLBACK
[0m14:15:16.264645 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:15:16.265643 [debug] [MainThread]: On master: Close
[0m14:15:16.433192 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:15:16.434224 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:15:16.434224 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:15:16.435189 [debug] [MainThread]: Connection 'model.jaffle_shop.my_second_dbt_model' was properly closed.
[0m14:15:16.437217 [info ] [MainThread]: 
[0m14:15:16.438181 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 10.75 seconds (10.75s).
[0m14:15:16.442170 [debug] [MainThread]: Command end result
[0m14:15:16.462117 [info ] [MainThread]: 
[0m14:15:16.463115 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:15:16.465109 [info ] [MainThread]: 
[0m14:15:16.467103 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:15:16.470123 [debug] [MainThread]: Command `dbt run` succeeded at 14:15:16.469105 after 16.48 seconds
[0m14:15:16.471092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6690197D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C66421F490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C66421F450>]}
[0m14:15:16.475082 [debug] [MainThread]: Flushing usage events
[0m14:23:54.737931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BB10D6F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BB0DF0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BB10D6C50>]}


============================== 14:23:54.743917 | f722b746-a501-41cf-9093-1c86b8da164e ==============================
[0m14:23:54.743917 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:23:54.745910 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:23:56.983646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BB1097050>]}
[0m14:23:57.005588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BC283E710>]}
[0m14:23:57.034475 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:23:57.051430 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m14:23:57.053482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BC2966490>]}
[0m14:23:58.511575 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m14:23:58.530526 [debug] [MainThread]: 1699: static parser successfully parsed stg_customers.sql
[0m14:23:58.536548 [debug] [MainThread]: 1699: static parser successfully parsed stg_orders.sql
[0m14:23:58.606359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BC291C2D0>]}
[0m14:23:58.622316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BC2996950>]}
[0m14:23:58.623311 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:23:58.624076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BC2959810>]}
[0m14:23:58.626076 [info ] [MainThread]: 
[0m14:23:58.629067 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:23:58.633056 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:23:58.652041 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:23:58.652041 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:23:58.653036 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:23:59.482271 [debug] [ThreadPool]: SQL status: OK in 0.8299999833106995 seconds
[0m14:23:59.487599 [debug] [ThreadPool]: On list_schemas: Close
[0m14:23:59.652239 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:23:59.662214 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:23:59.663213 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:23:59.664209 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:23:59.665207 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:00.236367 [debug] [ThreadPool]: SQL status: OK in 0.5699999928474426 seconds
[0m14:24:00.244903 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:24:00.244903 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:24:00.650094 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m14:24:00.653443 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:24:00.654438 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:24:00.654438 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:24:00.817861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BC27CCA50>]}
[0m14:24:00.818859 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:00.819856 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:00.820854 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:24:00.822859 [info ] [MainThread]: 
[0m14:24:00.830829 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_customers
[0m14:24:00.833827 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw.stg_customers ............................. [RUN]
[0m14:24:00.838811 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.stg_customers'
[0m14:24:00.840801 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_customers
[0m14:24:00.843793 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m14:24:00.850787 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (compile): 14:24:00.841798 => 14:24:00.849782
[0m14:24:00.852771 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_customers
[0m14:24:00.939642 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m14:24:00.941504 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:00.942503 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m14:24:00.942503 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */

  
    
        create or replace table `test_rw`.`stg_customers`
      
      
    using delta
      
      
      
      
      
      
      as
      select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers
  
[0m14:24:00.943500 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:24:03.568448 [debug] [Thread-1 (]: SQL status: OK in 2.619999885559082 seconds
[0m14:24:03.598418 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (execute): 14:24:00.854763 => 14:24:03.597414
[0m14:24:03.599423 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m14:24:03.599423 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:03.600401 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: Close
[0m14:24:03.761775 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BB1543A90>]}
[0m14:24:03.762773 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw.stg_customers ........................ [[32mOK[0m in 2.93s]
[0m14:24:03.764768 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_customers
[0m14:24:03.765766 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_orders
[0m14:24:03.766761 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw.stg_orders ................................ [RUN]
[0m14:24:03.768756 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_customers, now model.jaffle_shop.stg_orders)
[0m14:24:03.768756 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_orders
[0m14:24:03.771749 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m14:24:03.776737 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (compile): 14:24:03.769753 => 14:24:03.775740
[0m14:24:03.777734 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_orders
[0m14:24:03.788734 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m14:24:03.792575 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:03.793575 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m14:24:03.793575 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */

  
    
        create or replace table `test_rw`.`stg_orders`
      
      
    using delta
      
      
      
      
      
      
      as
      select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders
  
[0m14:24:03.794572 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:04.406963 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */

  
    
        create or replace table `test_rw`.`stg_orders`
      
      
    using delta
      
      
      
      
      
      
      as
      select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders
  
[0m14:24:04.407960 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 22 pos 5
[0m14:24:04.408955 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 22 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:609)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:501)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:361)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:339)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:324)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:373)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 22 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:586)
	... 20 more

[0m14:24:04.409955 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x01\xed\xf5\x7fB]\x1f0\x8ep\xaa\xd5\x81\x7fw8'
[0m14:24:04.411949 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (execute): 14:24:03.777734 => 14:24:04.410953
[0m14:24:04.412946 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m14:24:04.413944 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:04.414939 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: Close
[0m14:24:04.590061 [debug] [Thread-1 (]: Runtime Error in model stg_orders (models\stg_orders.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 22 pos 5
[0m14:24:04.591056 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f722b746-a501-41cf-9093-1c86b8da164e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BC287BD10>]}
[0m14:24:04.592054 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model test_rw.stg_orders ....................... [[31mERROR[0m in 0.82s]
[0m14:24:04.593013 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_orders
[0m14:24:04.595009 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:24:04.596005 [info ] [Thread-1 (]: 3 of 3 SKIP relation test_rw.customers ......................................... [[33mSKIP[0m]
[0m14:24:04.598002 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:24:04.599994 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:04.600991 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:04.808304 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:04.809300 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:04.809300 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:04.810337 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:04.810337 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:04.811330 [debug] [MainThread]: On master: Close
[0m14:24:05.022355 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:05.022355 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:24:05.023409 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:24:05.023409 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_orders' was properly closed.
[0m14:24:05.025378 [info ] [MainThread]: 
[0m14:24:05.026343 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 6.40 seconds (6.40s).
[0m14:24:05.027340 [debug] [MainThread]: Command end result
[0m14:24:05.051287 [info ] [MainThread]: 
[0m14:24:05.053272 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:24:05.054269 [info ] [MainThread]: 
[0m14:24:05.056302 [error] [MainThread]: [33mRuntime Error in model stg_orders (models\stg_orders.sql)[0m
[0m14:24:05.057261 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `jaffle_shop_orders` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m14:24:05.058256 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m14:24:05.059254 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 22 pos 5
[0m14:24:05.060252 [info ] [MainThread]: 
[0m14:24:05.061249 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m14:24:05.064260 [debug] [MainThread]: Command `dbt run` failed at 14:24:05.063420 after 10.40 seconds
[0m14:24:05.065242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BB0DF3B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BB0E82910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BAC459650>]}
[0m14:24:05.068232 [debug] [MainThread]: Flushing usage events
[0m14:24:35.936669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D222711D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2229E9FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D222788DD0>]}


============================== 14:24:35.940658 | 2f88c903-ae8b-4dd5-a01a-8dad74060df1 ==============================
[0m14:24:35.940658 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:24:35.942821 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:24:37.488712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2226F1D90>]}
[0m14:24:37.509666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2341452D0>]}
[0m14:24:37.535553 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:24:37.706149 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:24:37.707129 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\stg_orders.sql
[0m14:24:37.741005 [debug] [MainThread]: 1699: static parser successfully parsed stg_orders.sql
[0m14:24:37.781968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D234247A10>]}
[0m14:24:37.799882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2341F90D0>]}
[0m14:24:37.800871 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:24:37.801841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2341FB1D0>]}
[0m14:24:37.803874 [info ] [MainThread]: 
[0m14:24:37.805830 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:24:37.809823 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:24:37.827773 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:24:37.828771 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:24:37.831761 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:38.378343 [debug] [ThreadPool]: SQL status: OK in 0.550000011920929 seconds
[0m14:24:38.384619 [debug] [ThreadPool]: On list_schemas: Close
[0m14:24:38.544784 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:24:38.554761 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:38.555759 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:24:38.555759 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:24:38.556756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:39.105549 [debug] [ThreadPool]: SQL status: OK in 0.550000011920929 seconds
[0m14:24:39.119509 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:24:39.120506 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:24:39.493916 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m14:24:39.497908 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:24:39.498933 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:24:39.499882 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:24:39.677168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2340FC290>]}
[0m14:24:39.678168 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:39.678168 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:39.679126 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:24:39.681122 [info ] [MainThread]: 
[0m14:24:39.688104 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_customers
[0m14:24:39.690120 [info ] [Thread-1 (]: 1 of 3 START sql table model test_rw.stg_customers ............................. [RUN]
[0m14:24:39.692091 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.stg_customers'
[0m14:24:39.693125 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_customers
[0m14:24:39.695117 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m14:24:39.699107 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (compile): 14:24:39.694084 => 14:24:39.698132
[0m14:24:39.700069 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_customers
[0m14:24:39.723010 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:39.724044 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m14:24:39.725013 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */

      describe extended `test_rw`.`stg_customers`
  
[0m14:24:39.725013 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:24:40.336383 [debug] [Thread-1 (]: SQL status: OK in 0.6100000143051147 seconds
[0m14:24:40.385110 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m14:24:40.387105 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m14:24:40.387105 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */

  
    
        create or replace table `test_rw`.`stg_customers`
      
      
    using delta
      
      
      
      
      
      
      as
      select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers
  
[0m14:24:42.512298 [debug] [Thread-1 (]: SQL status: OK in 2.119999885559082 seconds
[0m14:24:42.570141 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (execute): 14:24:39.701096 => 14:24:42.569144
[0m14:24:42.571139 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m14:24:42.572136 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:42.574130 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: Close
[0m14:24:42.747669 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2343C6850>]}
[0m14:24:42.749661 [info ] [Thread-1 (]: 1 of 3 OK created sql table model test_rw.stg_customers ........................ [[32mOK[0m in 3.06s]
[0m14:24:42.750660 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_customers
[0m14:24:42.752652 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_orders
[0m14:24:42.753650 [info ] [Thread-1 (]: 2 of 3 START sql table model test_rw.stg_orders ................................ [RUN]
[0m14:24:42.755644 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_customers, now model.jaffle_shop.stg_orders)
[0m14:24:42.756642 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_orders
[0m14:24:42.760639 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m14:24:42.765622 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (compile): 14:24:42.757638 => 14:24:42.764643
[0m14:24:42.766616 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_orders
[0m14:24:42.779587 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m14:24:42.785575 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:42.787559 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m14:24:42.789561 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */

  
    
        create or replace table `test_rw`.`stg_orders`
      
      
    using delta
      
      
      
      
      
      
      as
      select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from test_rw.jaffle_shop_orders
  
[0m14:24:42.792558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:45.246899 [debug] [Thread-1 (]: SQL status: OK in 2.450000047683716 seconds
[0m14:24:45.250918 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (execute): 14:24:42.767613 => 14:24:45.250918
[0m14:24:45.251913 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m14:24:45.251913 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:45.252901 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: Close
[0m14:24:45.411429 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2343A9990>]}
[0m14:24:45.412399 [info ] [Thread-1 (]: 2 of 3 OK created sql table model test_rw.stg_orders ........................... [[32mOK[0m in 2.66s]
[0m14:24:45.414353 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_orders
[0m14:24:45.415380 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:24:45.416384 [info ] [Thread-1 (]: 3 of 3 START sql table model test_rw.customers ................................. [RUN]
[0m14:24:45.418341 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_orders, now model.jaffle_shop.customers)
[0m14:24:45.419375 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:24:45.426321 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:24:45.428314 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:24:45.419375 => 14:24:45.428314
[0m14:24:45.428314 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:24:45.436294 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:45.436294 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:24:45.437293 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

      describe extended `test_rw`.`customers`
  
[0m14:24:45.438299 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:45.977141 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m14:24:45.985113 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m14:24:45.986110 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:24:45.987107 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

  
    
        create or replace table `test_rw`.`customers`
      
      
    using delta
      
      
      
      
      
      
      as
      with customers as (

    select * from `test_rw`.`stg_customers`

),

orders as (

    select * from `test_rw`.`stg_orders`

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
  
[0m14:24:48.472461 [debug] [Thread-1 (]: SQL status: OK in 2.4800000190734863 seconds
[0m14:24:48.477447 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:24:45.429346 => 14:24:48.477447
[0m14:24:48.478445 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m14:24:48.479442 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:48.480438 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m14:24:48.638416 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f88c903-ae8b-4dd5-a01a-8dad74060df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2343A90D0>]}
[0m14:24:48.639413 [info ] [Thread-1 (]: 3 of 3 OK created sql table model test_rw.customers ............................ [[32mOK[0m in 3.22s]
[0m14:24:48.641411 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:24:48.645399 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:48.646395 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:48.880768 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:48.881764 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:48.881764 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:48.882761 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:48.883758 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:48.883758 [debug] [MainThread]: On master: Close
[0m14:24:49.043366 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:49.043366 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:24:49.044331 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:24:49.044331 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m14:24:49.046325 [info ] [MainThread]: 
[0m14:24:49.048318 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 11.24 seconds (11.24s).
[0m14:24:49.051311 [debug] [MainThread]: Command end result
[0m14:24:49.073252 [info ] [MainThread]: 
[0m14:24:49.075246 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:24:49.076244 [info ] [MainThread]: 
[0m14:24:49.078241 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:24:49.082228 [debug] [MainThread]: Command `dbt run` succeeded at 14:24:49.081241 after 13.20 seconds
[0m14:24:49.083246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2229D91D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D21DD795D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D22278BD10>]}
[0m14:24:49.085220 [debug] [MainThread]: Flushing usage events
[0m14:27:47.083757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E407D66850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4075EBA50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E403D35110>]}


============================== 14:27:47.088750 | 54c334ed-d249-4bea-b9ce-36c3456847cb ==============================
[0m14:27:47.088750 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:27:47.089361 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:27:48.416381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '54c334ed-d249-4bea-b9ce-36c3456847cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E407FA3ED0>]}
[0m14:27:48.441315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '54c334ed-d249-4bea-b9ce-36c3456847cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E419296590>]}
[0m14:27:48.472233 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:27:48.681678 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:27:48.681678 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:27:48.692648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '54c334ed-d249-4bea-b9ce-36c3456847cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4193DE4D0>]}
[0m14:27:48.710599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '54c334ed-d249-4bea-b9ce-36c3456847cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4192C6650>]}
[0m14:27:48.711596 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:27:48.712593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54c334ed-d249-4bea-b9ce-36c3456847cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E419296910>]}
[0m14:27:48.715587 [info ] [MainThread]: 
[0m14:27:48.717581 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:27:48.722568 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:27:48.745506 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:27:48.746504 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:48.747500 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:49.515817 [debug] [ThreadPool]: SQL status: OK in 0.7699999809265137 seconds
[0m14:27:49.525790 [debug] [ThreadPool]: On list_schemas: Close
[0m14:27:49.692348 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:27:49.706310 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:49.706310 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:27:49.707308 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:27:49.708305 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:50.319358 [debug] [ThreadPool]: SQL status: OK in 0.6100000143051147 seconds
[0m14:27:50.330330 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:27:50.331328 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:27:50.699647 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m14:27:50.704669 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:27:50.705660 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:27:50.706664 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:27:50.867204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54c334ed-d249-4bea-b9ce-36c3456847cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4192BD210>]}
[0m14:27:50.869197 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:50.870194 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:27:50.871196 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:27:50.873186 [info ] [MainThread]: 
[0m14:27:50.885157 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:27:50.889148 [info ] [Thread-1 (]: 1 of 1 START sql table model test_rw.customers ................................. [RUN]
[0m14:27:50.893164 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.customers'
[0m14:27:50.894130 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:27:50.899119 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:27:50.905107 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:27:50.895127 => 14:27:50.903107
[0m14:27:50.907097 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:27:50.946991 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:50.947988 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:27:50.948985 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

      describe extended `test_rw`.`customers`
  
[0m14:27:50.949984 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:27:51.548456 [debug] [Thread-1 (]: SQL status: OK in 0.6000000238418579 seconds
[0m14:27:51.901029 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m14:27:51.904024 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.customers"
[0m14:27:51.904024 [debug] [Thread-1 (]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */

  
    
        create or replace table `test_rw`.`customers`
      
      
    using delta
      
      
      
      
      
      
      as
      with customers as (

    select * from `test_rw`.`stg_customers`

),

orders as (

    select * from `test_rw`.`stg_orders`

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
  
[0m14:27:54.073663 [debug] [Thread-1 (]: SQL status: OK in 2.1700000762939453 seconds
[0m14:27:54.123530 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:27:50.908093 => 14:27:54.122533
[0m14:27:54.124527 [debug] [Thread-1 (]: On model.jaffle_shop.customers: ROLLBACK
[0m14:27:54.125529 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:54.126522 [debug] [Thread-1 (]: On model.jaffle_shop.customers: Close
[0m14:27:54.287448 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '54c334ed-d249-4bea-b9ce-36c3456847cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E419382150>]}
[0m14:27:54.288434 [info ] [Thread-1 (]: 1 of 1 OK created sql table model test_rw.customers ............................ [[32mOK[0m in 3.40s]
[0m14:27:54.290427 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:27:54.293421 [debug] [MainThread]: On master: ROLLBACK
[0m14:27:54.295419 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:27:54.514112 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:27:54.515110 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:54.516108 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:27:54.516108 [debug] [MainThread]: On master: ROLLBACK
[0m14:27:54.517104 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:27:54.518103 [debug] [MainThread]: On master: Close
[0m14:27:54.687651 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:54.688650 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:27:54.689647 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:27:54.689647 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m14:27:54.691642 [info ] [MainThread]: 
[0m14:27:54.692639 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.98 seconds (5.98s).
[0m14:27:54.694633 [debug] [MainThread]: Command end result
[0m14:27:54.714582 [info ] [MainThread]: 
[0m14:27:54.715576 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:27:54.716578 [info ] [MainThread]: 
[0m14:27:54.719617 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:27:54.724584 [debug] [MainThread]: Command `dbt run` succeeded at 14:27:54.724584 after 7.69 seconds
[0m14:27:54.725550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E407FA3150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E407BE4150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E407B17FD0>]}
[0m14:27:54.726548 [debug] [MainThread]: Flushing usage events
[0m14:28:44.523761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1A954A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1A6702D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1AD2F550>]}


============================== 14:28:44.528781 | 41e77d2c-2b6f-42f4-a671-1f96b81bc9f8 ==============================
[0m14:28:44.528781 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:28:44.529745 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:28:46.652097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1A5F5350>]}
[0m14:28:46.683015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2C005990>]}
[0m14:28:46.719918 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:28:46.918418 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m14:28:46.919387 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\staging\stg_customers.sql
[0m14:28:46.920387 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\staging\stg_orders.sql
[0m14:28:46.921384 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models\stg_orders.sql
[0m14:28:46.922381 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models\stg_customers.sql
[0m14:28:46.967262 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_customers.sql
[0m14:28:46.994190 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_orders.sql
[0m14:28:47.002169 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m14:28:47.029097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2C069410>]}
[0m14:28:47.049045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2C0BD610>]}
[0m14:28:47.050040 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:28:47.052036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2C0BF8D0>]}
[0m14:28:47.055027 [info ] [MainThread]: 
[0m14:28:47.057022 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:28:47.062010 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m14:28:47.085944 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m14:28:47.088937 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:28:47.089933 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:47.676185 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m14:28:47.684162 [debug] [ThreadPool]: On list_schemas: Close
[0m14:28:47.855704 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:28:47.868670 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:47.871663 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:28:47.871663 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:28:47.872660 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:48.405875 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m14:28:48.419816 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:28:48.420813 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:28:48.776865 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m14:28:48.781851 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:28:48.783881 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:28:48.783881 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:28:48.951435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2C093490>]}
[0m14:28:48.952433 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:48.953398 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:28:48.954421 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:28:48.956387 [info ] [MainThread]: 
[0m14:28:48.964404 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_customers
[0m14:28:48.966362 [info ] [Thread-1 (]: 1 of 2 START sql table model test_rw.stg_customers ............................. [RUN]
[0m14:28:48.970353 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.stg_customers'
[0m14:28:48.971348 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_customers
[0m14:28:48.975365 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m14:28:48.979329 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (compile): 14:28:48.972345 => 14:28:48.979329
[0m14:28:48.981322 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_customers
[0m14:28:49.011284 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:49.012239 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m14:28:49.013236 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */

      describe extended `test_rw`.`stg_customers`
  
[0m14:28:49.013236 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:28:49.618841 [debug] [Thread-1 (]: SQL status: OK in 0.6100000143051147 seconds
[0m14:28:49.688655 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m14:28:49.691647 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m14:28:49.692646 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */

  
    
        create or replace table `test_rw`.`stg_customers`
      
      
    using delta
      
      
      
      
      
      
      as
      select
        id as customer_id,
        first_name,
        last_name

    from test_rw.jaffle_shop_customers
  
[0m14:28:51.621413 [debug] [Thread-1 (]: SQL status: OK in 1.9299999475479126 seconds
[0m14:28:51.666281 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (execute): 14:28:48.982360 => 14:28:51.665284
[0m14:28:51.667279 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m14:28:51.669274 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:51.670271 [debug] [Thread-1 (]: On model.jaffle_shop.stg_customers: Close
[0m14:28:51.836828 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2C28A050>]}
[0m14:28:51.837827 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test_rw.stg_customers ........................ [[32mOK[0m in 2.87s]
[0m14:28:51.838824 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_customers
[0m14:28:51.839822 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_orders
[0m14:28:51.841817 [info ] [Thread-1 (]: 2 of 2 START sql table model test_rw.stg_orders ................................ [RUN]
[0m14:28:51.844807 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_customers, now model.jaffle_shop.stg_orders)
[0m14:28:51.845803 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_orders
[0m14:28:51.847798 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m14:28:51.849793 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (compile): 14:28:51.845803 => 14:28:51.849793
[0m14:28:51.850791 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_orders
[0m14:28:51.857772 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:51.858770 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m14:28:51.859807 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */

      describe extended `test_rw`.`stg_orders`
  
[0m14:28:51.859807 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:52.422577 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m14:28:52.436540 [debug] [Thread-1 (]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m14:28:52.439532 [debug] [Thread-1 (]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m14:28:52.441526 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */

  
    
        create or replace table `test_rw`.`stg_orders`
      
      
    using delta
      
      
      
      
      
      
      as
      select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from test_rw.jaffle_shop_orders
  
[0m14:28:54.292820 [debug] [Thread-1 (]: SQL status: OK in 1.850000023841858 seconds
[0m14:28:54.297807 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (execute): 14:28:51.850791 => 14:28:54.297807
[0m14:28:54.298804 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m14:28:54.299802 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:54.299802 [debug] [Thread-1 (]: On model.jaffle_shop.stg_orders: Close
[0m14:28:54.466190 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41e77d2c-2b6f-42f4-a671-1f96b81bc9f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2C2ADAD0>]}
[0m14:28:54.467186 [info ] [Thread-1 (]: 2 of 2 OK created sql table model test_rw.stg_orders ........................... [[32mOK[0m in 2.62s]
[0m14:28:54.467735 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_orders
[0m14:28:54.470771 [debug] [MainThread]: On master: ROLLBACK
[0m14:28:54.471756 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:28:54.667950 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:28:54.668951 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:54.668951 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:28:54.669947 [debug] [MainThread]: On master: ROLLBACK
[0m14:28:54.670944 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:28:54.670944 [debug] [MainThread]: On master: Close
[0m14:28:54.839477 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:28:54.839477 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:28:54.840475 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:28:54.840475 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_orders' was properly closed.
[0m14:28:54.842469 [info ] [MainThread]: 
[0m14:28:54.843434 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 7.79 seconds (7.79s).
[0m14:28:54.846427 [debug] [MainThread]: Command end result
[0m14:28:54.866374 [info ] [MainThread]: 
[0m14:28:54.868366 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:28:54.869366 [info ] [MainThread]: 
[0m14:28:54.872356 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:28:54.877352 [debug] [MainThread]: Command `dbt run` succeeded at 14:28:54.876345 after 10.41 seconds
[0m14:28:54.878340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1A673010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA159BF550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA15C49650>]}
[0m14:28:54.879338 [debug] [MainThread]: Flushing usage events
[0m14:30:02.418576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDE112EB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDE0CE3850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDE112E590>]}


============================== 14:30:02.427551 | be692e8c-d289-4434-881b-23b849dfc8a1 ==============================
[0m14:30:02.427551 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:30:02.430543 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:30:05.182729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be692e8c-d289-4434-881b-23b849dfc8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDE112EF90>]}
[0m14:30:05.209657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be692e8c-d289-4434-881b-23b849dfc8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF2417FD0>]}
[0m14:30:05.254539 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:30:05.473671 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m14:30:05.475665 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\schema.yml
[0m14:30:05.681118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be692e8c-d289-4434-881b-23b849dfc8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF24E6810>]}
[0m14:30:05.710041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be692e8c-d289-4434-881b-23b849dfc8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF264B490>]}
[0m14:30:05.712036 [info ] [MainThread]: Found 3 models, 9 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:30:05.714030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be692e8c-d289-4434-881b-23b849dfc8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF2494490>]}
[0m14:30:05.719020 [info ] [MainThread]: 
[0m14:30:05.722009 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:30:05.727994 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:30:05.764897 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:05.765893 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:30:05.766890 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:30:05.767886 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:30:06.537504 [debug] [ThreadPool]: SQL status: OK in 0.7699999809265137 seconds
[0m14:30:06.559448 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:30:06.560446 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:30:06.940796 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m14:30:06.945783 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:30:06.946779 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:30:06.947776 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:30:07.126301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be692e8c-d289-4434-881b-23b849dfc8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF24D2F10>]}
[0m14:30:07.127298 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:07.128296 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:30:07.130291 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:30:07.131288 [info ] [MainThread]: 
[0m14:30:07.142259 [debug] [Thread-1 (]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:30:07.144273 [info ] [Thread-1 (]: 1 of 9 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m14:30:07.149241 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad'
[0m14:30:07.151235 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:30:07.170187 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:30:07.179161 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (compile): 14:30:07.151235 => 14:30:07.178165
[0m14:30:07.181159 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:30:07.215066 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:30:07.220053 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:07.222048 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:30:07.225040 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `test_rw`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m14:30:07.226037 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:30:08.285122 [debug] [Thread-1 (]: SQL status: OK in 1.059999942779541 seconds
[0m14:30:08.293103 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (execute): 14:30:07.182153 => 14:30:08.292106
[0m14:30:08.294100 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: ROLLBACK
[0m14:30:08.295099 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:08.296095 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: Close
[0m14:30:08.460997 [info ] [Thread-1 (]: 1 of 9 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 1.31s]
[0m14:30:08.462989 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:30:08.463987 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:30:08.464984 [info ] [Thread-1 (]: 2 of 9 START test not_null_customers_customer_id ............................... [RUN]
[0m14:30:08.467975 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad, now test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d)
[0m14:30:08.468973 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:30:08.487920 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:30:08.495899 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (compile): 14:30:08.469970 => 14:30:08.494903
[0m14:30:08.497895 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:30:08.504876 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:30:08.512855 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:08.513852 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:30:08.514850 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:30:08.515846 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:09.229871 [debug] [Thread-1 (]: SQL status: OK in 0.7099999785423279 seconds
[0m14:30:09.236220 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (execute): 14:30:08.498891 => 14:30:09.235697
[0m14:30:09.236744 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: ROLLBACK
[0m14:30:09.237795 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:09.239461 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: Close
[0m14:30:09.405058 [info ] [Thread-1 (]: 2 of 9 PASS not_null_customers_customer_id ..................................... [[32mPASS[0m in 0.94s]
[0m14:30:09.408050 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:30:09.409049 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:30:09.410044 [info ] [Thread-1 (]: 3 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m14:30:09.412040 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d, now test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m14:30:09.413039 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:30:09.427997 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:30:09.432987 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 14:30:09.414036 => 14:30:09.431987
[0m14:30:09.435979 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:30:09.446954 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:30:09.452931 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:09.453930 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:30:09.454925 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:30:09.457918 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:10.228336 [debug] [Thread-1 (]: SQL status: OK in 0.7699999809265137 seconds
[0m14:30:10.233323 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 14:30:09.438969 => 14:30:10.233323
[0m14:30:10.234320 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m14:30:10.235318 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:10.236315 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m14:30:10.402402 [info ] [Thread-1 (]: 3 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 0.99s]
[0m14:30:10.405393 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:30:10.407398 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:30:10.408386 [info ] [Thread-1 (]: 4 of 9 START test not_null_stg_orders_customer_id .............................. [RUN]
[0m14:30:10.412376 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5)
[0m14:30:10.413372 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:30:10.423351 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:30:10.427335 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (compile): 14:30:10.414370 => 14:30:10.426338
[0m14:30:10.428335 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:30:10.434340 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:30:10.437311 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:10.439192 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:30:10.441299 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_orders`
where customer_id is null



      
    ) dbt_internal_test
[0m14:30:10.442295 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:11.086383 [debug] [Thread-1 (]: SQL status: OK in 0.6399999856948853 seconds
[0m14:30:11.091372 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (execute): 14:30:10.429330 => 14:30:11.091372
[0m14:30:11.092369 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: ROLLBACK
[0m14:30:11.093367 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:11.094365 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: Close
[0m14:30:11.250947 [info ] [Thread-1 (]: 4 of 9 PASS not_null_stg_orders_customer_id .................................... [[32mPASS[0m in 0.84s]
[0m14:30:11.253941 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:30:11.254935 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:30:11.255933 [info ] [Thread-1 (]: 5 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m14:30:11.257928 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5, now test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:30:11.258925 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:30:11.269896 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:30:11.274894 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (compile): 14:30:11.259922 => 14:30:11.273886
[0m14:30:11.276909 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:30:11.283887 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:30:11.286850 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:11.287847 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:30:11.288846 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `test_rw`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m14:30:11.291859 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:11.967313 [debug] [Thread-1 (]: SQL status: OK in 0.6800000071525574 seconds
[0m14:30:11.972297 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (execute): 14:30:11.278874 => 14:30:11.972297
[0m14:30:11.973293 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m14:30:11.974291 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:11.975288 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m14:30:12.144103 [info ] [Thread-1 (]: 5 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 0.89s]
[0m14:30:12.146101 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:30:12.147096 [debug] [Thread-1 (]: Began running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:30:12.148093 [info ] [Thread-1 (]: 6 of 9 START test relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [RUN]
[0m14:30:12.150089 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64, now test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500)
[0m14:30:12.151086 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:30:12.162058 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:30:12.167045 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (compile): 14:30:12.152084 => 14:30:12.166049
[0m14:30:12.168043 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:30:12.174029 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:30:12.178014 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:12.179010 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:30:12.181005 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from `test_rw`.`stg_orders`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `test_rw`.`stg_customers`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m14:30:12.183000 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:13.107531 [debug] [Thread-1 (]: SQL status: OK in 0.9300000071525574 seconds
[0m14:30:13.113515 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (execute): 14:30:12.169037 => 14:30:13.112518
[0m14:30:13.114520 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: ROLLBACK
[0m14:30:13.115511 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:13.116508 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: Close
[0m14:30:13.281468 [info ] [Thread-1 (]: 6 of 9 PASS relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [[32mPASS[0m in 1.13s]
[0m14:30:13.283464 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:30:13.284464 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:30:13.286458 [info ] [Thread-1 (]: 7 of 9 START test unique_customers_customer_id ................................. [RUN]
[0m14:30:13.288451 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500, now test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1)
[0m14:30:13.290445 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:30:13.313385 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:30:13.318371 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (compile): 14:30:13.291442 => 14:30:13.317373
[0m14:30:13.320368 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:30:13.414116 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:30:13.418105 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:13.419102 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:30:13.420100 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:30:13.421097 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:14.324291 [debug] [Thread-1 (]: SQL status: OK in 0.8999999761581421 seconds
[0m14:30:14.329278 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (execute): 14:30:13.321365 => 14:30:14.328280
[0m14:30:14.330276 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: ROLLBACK
[0m14:30:14.331270 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:14.332269 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: Close
[0m14:30:14.513785 [info ] [Thread-1 (]: 7 of 9 PASS unique_customers_customer_id ....................................... [[32mPASS[0m in 1.23s]
[0m14:30:14.515779 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:30:14.517776 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:30:14.519772 [info ] [Thread-1 (]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m14:30:14.523760 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1, now test.jaffle_shop.unique_stg_customers_customer_id.c7614daada)
[0m14:30:14.525754 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:30:14.536724 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:30:14.541711 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (compile): 14:30:14.526751 => 14:30:14.540713
[0m14:30:14.542708 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:30:14.549194 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:30:14.552984 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:14.556964 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:30:14.563942 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:30:14.564940 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:15.389505 [debug] [Thread-1 (]: SQL status: OK in 0.8199999928474426 seconds
[0m14:30:15.394495 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (execute): 14:30:14.543706 => 14:30:15.394495
[0m14:30:15.395489 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m14:30:15.396486 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:15.397484 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m14:30:15.560051 [info ] [Thread-1 (]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.04s]
[0m14:30:15.562049 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:30:15.564040 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:30:15.565036 [info ] [Thread-1 (]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m14:30:15.568030 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_customers_customer_id.c7614daada, now test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a)
[0m14:30:15.569025 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:30:15.582403 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:30:15.586049 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (compile): 14:30:15.570024 => 14:30:15.585533
[0m14:30:15.587082 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:30:15.595131 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:30:15.598122 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:15.600118 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:30:15.601123 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_orders`
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:30:15.603111 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:16.354189 [debug] [Thread-1 (]: SQL status: OK in 0.75 seconds
[0m14:30:16.360172 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (execute): 14:30:15.587599 => 14:30:16.359175
[0m14:30:16.361169 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m14:30:16.362168 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:16.363163 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m14:30:16.527861 [info ] [Thread-1 (]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 0.96s]
[0m14:30:16.529855 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:30:16.532847 [debug] [MainThread]: On master: ROLLBACK
[0m14:30:16.534842 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:30:16.774205 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:30:16.775204 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:16.775204 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:30:16.776201 [debug] [MainThread]: On master: ROLLBACK
[0m14:30:16.777198 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:30:16.778195 [debug] [MainThread]: On master: Close
[0m14:30:16.944753 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:30:16.945924 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:30:16.946499 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:30:16.948568 [info ] [MainThread]: 
[0m14:30:16.949601 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 11.23 seconds (11.23s).
[0m14:30:16.953754 [debug] [MainThread]: Command end result
[0m14:30:16.979338 [info ] [MainThread]: 
[0m14:30:16.981333 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:30:16.983330 [info ] [MainThread]: 
[0m14:30:16.985324 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m14:30:16.987318 [debug] [MainThread]: Command `dbt test` succeeded at 14:30:16.986322 after 14.66 seconds
[0m14:30:16.988317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDE0DB2910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDE0A5D850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDC0495D0>]}
[0m14:30:16.990345 [debug] [MainThread]: Flushing usage events
[0m14:32:42.243542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F965D88A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F965D99FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F96317EC90>]}


============================== 14:32:42.250524 | ca1585d5-4d48-40db-b251-0fc40c166de0 ==============================
[0m14:32:42.250524 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:32:42.252518 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:32:44.663092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca1585d5-4d48-40db-b251-0fc40c166de0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F965CF5210>]}
[0m14:32:44.696005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ca1585d5-4d48-40db-b251-0fc40c166de0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F97750D4D0>]}
[0m14:32:44.733902 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:32:44.980246 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:32:44.983238 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\schema.yml
[0m14:32:45.039089 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_orders.sql
[0m14:32:45.235564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca1585d5-4d48-40db-b251-0fc40c166de0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9774F1910>]}
[0m14:32:45.263492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca1585d5-4d48-40db-b251-0fc40c166de0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9777762D0>]}
[0m14:32:45.265487 [info ] [MainThread]: Found 3 models, 9 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:32:45.267480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca1585d5-4d48-40db-b251-0fc40c166de0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F977777FD0>]}
[0m14:32:45.272467 [info ] [MainThread]: 
[0m14:32:45.276457 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:32:45.283449 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:32:45.321337 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:45.323331 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:32:45.324328 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:32:45.327320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:46.091283 [debug] [ThreadPool]: SQL status: OK in 0.7599999904632568 seconds
[0m14:32:46.118212 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:32:46.120206 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:32:46.505181 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m14:32:46.511196 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:32:46.512198 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:32:46.513190 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:32:46.706642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca1585d5-4d48-40db-b251-0fc40c166de0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F97770F950>]}
[0m14:32:46.708638 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:46.709637 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:32:46.712627 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:32:46.713625 [info ] [MainThread]: 
[0m14:32:46.914090 [debug] [Thread-1 (]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:32:46.916084 [info ] [Thread-1 (]: 1 of 9 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending  [RUN]
[0m14:32:46.936031 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5'
[0m14:32:46.938030 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:32:46.958970 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"
[0m14:32:46.972933 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5 (compile): 14:32:46.940021 => 14:32:46.962965
[0m14:32:46.976932 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:32:47.021802 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"
[0m14:32:47.027786 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:47.029780 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"
[0m14:32:47.030778 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `test_rw`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending'
)



      
    ) dbt_internal_test
[0m14:32:47.032773 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:32:47.888523 [debug] [Thread-1 (]: SQL status: OK in 0.8600000143051147 seconds
[0m14:32:47.894476 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5 (execute): 14:32:46.979927 => 14:32:47.894476
[0m14:32:47.895502 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5: ROLLBACK
[0m14:32:47.896473 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:47.897503 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5: Close
[0m14:32:48.060034 [error] [Thread-1 (]: 1 of 9 FAIL 1 accepted_values_stg_orders_status__placed__shipped__completed__return_pending  [[31mFAIL 1[0m in 1.13s]
[0m14:32:48.062029 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:32:48.064023 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:32:48.065021 [info ] [Thread-1 (]: 2 of 9 START test not_null_customers_customer_id ............................... [RUN]
[0m14:32:48.067016 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5, now test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d)
[0m14:32:48.068013 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:32:48.086992 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:32:48.089954 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (compile): 14:32:48.069011 => 14:32:48.088959
[0m14:32:48.090951 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:32:48.096936 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:32:48.100927 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:48.101926 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:32:48.102923 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:32:48.105933 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:48.643478 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m14:32:48.648496 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (execute): 14:32:48.091949 => 14:32:48.648496
[0m14:32:48.649490 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: ROLLBACK
[0m14:32:48.650461 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:48.651485 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: Close
[0m14:32:48.826989 [info ] [Thread-1 (]: 2 of 9 PASS not_null_customers_customer_id ..................................... [[32mPASS[0m in 0.76s]
[0m14:32:48.828984 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:32:48.829981 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:32:48.830978 [info ] [Thread-1 (]: 3 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m14:32:48.832973 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d, now test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m14:32:48.833971 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:32:48.848932 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:32:48.852920 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 14:32:48.835967 => 14:32:48.851922
[0m14:32:48.853922 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:32:48.858904 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:32:48.864888 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:48.865885 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:32:48.867883 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:32:48.868880 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:49.417415 [debug] [Thread-1 (]: SQL status: OK in 0.550000011920929 seconds
[0m14:32:49.423419 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 14:32:48.854918 => 14:32:49.423419
[0m14:32:49.424398 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m14:32:49.426391 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:49.427391 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m14:32:49.608905 [info ] [Thread-1 (]: 3 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 0.78s]
[0m14:32:49.611897 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:32:49.613891 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:32:49.614888 [info ] [Thread-1 (]: 4 of 9 START test not_null_stg_orders_customer_id .............................. [RUN]
[0m14:32:49.617880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5)
[0m14:32:49.619875 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:32:49.634836 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:32:49.637829 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (compile): 14:32:49.620873 => 14:32:49.637829
[0m14:32:49.638826 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:32:49.647803 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:32:49.651790 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:49.652787 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:32:49.653785 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_orders`
where customer_id is null



      
    ) dbt_internal_test
[0m14:32:49.655779 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:50.212295 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m14:32:50.218279 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (execute): 14:32:49.639824 => 14:32:50.218279
[0m14:32:50.220274 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: ROLLBACK
[0m14:32:50.221271 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:50.222269 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: Close
[0m14:32:50.389822 [info ] [Thread-1 (]: 4 of 9 PASS not_null_stg_orders_customer_id .................................... [[32mPASS[0m in 0.77s]
[0m14:32:50.392816 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:32:50.394809 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:32:50.396805 [info ] [Thread-1 (]: 5 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m14:32:50.400793 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5, now test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:32:50.401790 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:32:50.423734 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:32:50.430716 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (compile): 14:32:50.402787 => 14:32:50.428725
[0m14:32:50.432708 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:32:50.437695 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:32:50.439689 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:50.440687 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:32:50.442939 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `test_rw`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m14:32:50.445674 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:51.096903 [debug] [Thread-1 (]: SQL status: OK in 0.6499999761581421 seconds
[0m14:32:51.101888 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (execute): 14:32:50.433706 => 14:32:51.101888
[0m14:32:51.102885 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m14:32:51.104880 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:51.105878 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m14:32:51.284402 [info ] [Thread-1 (]: 5 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 0.88s]
[0m14:32:51.286399 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:32:51.288393 [debug] [Thread-1 (]: Began running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:32:51.289389 [info ] [Thread-1 (]: 6 of 9 START test relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [RUN]
[0m14:32:51.292381 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64, now test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500)
[0m14:32:51.293378 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:32:51.308367 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:32:51.315319 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (compile): 14:32:51.295373 => 14:32:51.314322
[0m14:32:51.316317 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:32:51.324296 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:32:51.332278 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:51.333272 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:32:51.334269 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from `test_rw`.`stg_orders`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `test_rw`.`stg_customers`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m14:32:51.335266 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:51.912633 [debug] [Thread-1 (]: SQL status: OK in 0.5799999833106995 seconds
[0m14:32:51.917620 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (execute): 14:32:51.317314 => 14:32:51.916623
[0m14:32:51.918617 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: ROLLBACK
[0m14:32:51.919615 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:51.920612 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: Close
[0m14:32:52.092154 [info ] [Thread-1 (]: 6 of 9 PASS relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [[32mPASS[0m in 0.80s]
[0m14:32:52.095147 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:32:52.097144 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:32:52.098141 [info ] [Thread-1 (]: 7 of 9 START test unique_customers_customer_id ................................. [RUN]
[0m14:32:52.101132 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500, now test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1)
[0m14:32:52.103127 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:32:52.122076 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:32:52.127066 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (compile): 14:32:52.103127 => 14:32:52.125067
[0m14:32:52.128907 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:32:52.136877 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:32:52.140867 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:52.143871 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:32:52.145857 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:52.147847 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:52.680499 [debug] [Thread-1 (]: SQL status: OK in 0.5299999713897705 seconds
[0m14:32:52.685487 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (execute): 14:32:52.130893 => 14:32:52.685487
[0m14:32:52.686484 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: ROLLBACK
[0m14:32:52.687487 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:52.688480 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: Close
[0m14:32:52.858026 [info ] [Thread-1 (]: 7 of 9 PASS unique_customers_customer_id ....................................... [[32mPASS[0m in 0.76s]
[0m14:32:52.861019 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:32:52.862018 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:32:52.863014 [info ] [Thread-1 (]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m14:32:52.867011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1, now test.jaffle_shop.unique_stg_customers_customer_id.c7614daada)
[0m14:32:52.868001 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:32:52.879968 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:32:52.882960 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (compile): 14:32:52.869021 => 14:32:52.881962
[0m14:32:52.883958 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:32:52.891935 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:32:52.896174 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:52.898167 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:32:52.900171 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:52.902156 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:53.432779 [debug] [Thread-1 (]: SQL status: OK in 0.5299999713897705 seconds
[0m14:32:53.436765 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (execute): 14:32:52.884956 => 14:32:53.436765
[0m14:32:53.437768 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m14:32:53.438726 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:53.439754 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m14:32:53.601290 [info ] [Thread-1 (]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 0.74s]
[0m14:32:53.603286 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:32:53.603286 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:32:53.604283 [info ] [Thread-1 (]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m14:32:53.606278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_customers_customer_id.c7614daada, now test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a)
[0m14:32:53.607275 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:32:53.619245 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:32:53.621238 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (compile): 14:32:53.608272 => 14:32:53.621238
[0m14:32:53.622237 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:32:53.627231 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:32:53.632209 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:53.633206 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:32:53.634203 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_orders`
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:53.635201 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:54.177754 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m14:32:54.181743 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (execute): 14:32:53.622237 => 14:32:54.180746
[0m14:32:54.182740 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m14:32:54.182740 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:54.183738 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m14:32:54.344092 [info ] [Thread-1 (]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 0.74s]
[0m14:32:54.346085 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:32:54.349078 [debug] [MainThread]: On master: ROLLBACK
[0m14:32:54.350074 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:32:54.557522 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:32:54.558553 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:54.559517 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:32:54.560514 [debug] [MainThread]: On master: ROLLBACK
[0m14:32:54.560514 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:32:54.561510 [debug] [MainThread]: On master: Close
[0m14:32:54.719090 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:32:54.720087 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:32:54.721085 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:32:54.722082 [info ] [MainThread]: 
[0m14:32:54.723080 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 9.45 seconds (9.45s).
[0m14:32:54.727069 [debug] [MainThread]: Command end result
[0m14:32:54.752004 [info ] [MainThread]: 
[0m14:32:54.753002 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:32:54.754996 [info ] [MainThread]: 
[0m14:32:54.755994 [error] [MainThread]: [31mFailure in test accepted_values_stg_orders_status__placed__shipped__completed__return_pending (models\schema.yml)[0m
[0m14:32:54.756992 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:32:54.757988 [info ] [MainThread]: 
[0m14:32:54.761977 [info ] [MainThread]:   compiled Code at target\compiled\jaffle_shop\models\schema.yml\accepted_values_stg_orders_62d7846404b81142bf3afb7b27ee2a82.sql
[0m14:32:54.764972 [info ] [MainThread]: 
[0m14:32:54.766963 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=1 SKIP=0 TOTAL=9
[0m14:32:54.768957 [debug] [MainThread]: Command `dbt test` failed at 14:32:54.768957 after 12.62 seconds
[0m14:32:54.769955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F965E27F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9661FE150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F961129650>]}
[0m14:32:54.770952 [debug] [MainThread]: Flushing usage events
[0m14:33:17.783973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736F206F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736EEE7690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736EF13090>]}


============================== 14:33:17.791985 | 3cc3aa6e-e3e1-445c-860a-f11dc08e4885 ==============================
[0m14:33:17.791985 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:33:17.793949 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:33:20.140691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3cc3aa6e-e3e1-445c-860a-f11dc08e4885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736F1F6E90>]}
[0m14:33:20.167618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3cc3aa6e-e3e1-445c-860a-f11dc08e4885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000173009BCA90>]}
[0m14:33:20.203525 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:33:20.416953 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m14:33:20.417951 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\badger.yml
[0m14:33:20.466820 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m14:33:20.496740 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_orders.sql
[0m14:33:20.504720 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_customers.sql
[0m14:33:20.686265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3cc3aa6e-e3e1-445c-860a-f11dc08e4885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017300B5AF10>]}
[0m14:33:20.710172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3cc3aa6e-e3e1-445c-860a-f11dc08e4885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017300C2EDD0>]}
[0m14:33:20.712166 [info ] [MainThread]: Found 3 models, 9 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:33:20.713166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cc3aa6e-e3e1-445c-860a-f11dc08e4885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017300A1CB10>]}
[0m14:33:20.717153 [info ] [MainThread]: 
[0m14:33:20.720174 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:33:20.724134 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:33:20.755067 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:20.756083 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:33:20.757048 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:33:20.758044 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:21.341516 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m14:33:21.360433 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:33:21.361430 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:33:21.726456 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m14:33:21.732441 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:33:21.733438 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:33:21.733438 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:33:21.904689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cc3aa6e-e3e1-445c-860a-f11dc08e4885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736EF11950>]}
[0m14:33:21.905687 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:21.906712 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:33:21.907683 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:21.909676 [info ] [MainThread]: 
[0m14:33:21.919649 [debug] [Thread-1 (]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:33:21.921689 [info ] [Thread-1 (]: 1 of 9 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending  [RUN]
[0m14:33:21.924637 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5'
[0m14:33:21.926969 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:33:21.938601 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"
[0m14:33:21.946577 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5 (compile): 14:33:21.928629 => 14:33:21.945580
[0m14:33:21.947575 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:33:21.973624 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"
[0m14:33:21.980605 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:21.981604 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"
[0m14:33:21.982601 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `test_rw`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending'
)



      
    ) dbt_internal_test
[0m14:33:21.984597 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:33:22.525566 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m14:33:22.531550 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5 (execute): 14:33:21.947575 => 14:33:22.531550
[0m14:33:22.532548 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5: ROLLBACK
[0m14:33:22.533545 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:22.533545 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5: Close
[0m14:33:22.721079 [error] [Thread-1 (]: 1 of 9 FAIL 1 accepted_values_stg_orders_status__placed__shipped__completed__return_pending  [[31mFAIL 1[0m in 0.80s]
[0m14:33:22.723040 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5
[0m14:33:22.724035 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:22.725033 [info ] [Thread-1 (]: 2 of 9 START test not_null_customers_customer_id ............................... [RUN]
[0m14:33:22.727031 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending.53fa0900e5, now test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d)
[0m14:33:22.728028 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:22.747981 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:33:22.750967 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (compile): 14:33:22.729024 => 14:33:22.750967
[0m14:33:22.751963 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:22.755952 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:33:22.758950 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:22.759943 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:33:22.762942 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:33:22.764939 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:23.282549 [debug] [Thread-1 (]: SQL status: OK in 0.5199999809265137 seconds
[0m14:33:23.287533 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (execute): 14:33:22.752964 => 14:33:23.287533
[0m14:33:23.288531 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: ROLLBACK
[0m14:33:23.289529 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:23.290525 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: Close
[0m14:33:23.454090 [info ] [Thread-1 (]: 2 of 9 PASS not_null_customers_customer_id ..................................... [[32mPASS[0m in 0.73s]
[0m14:33:23.457081 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:23.458081 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:23.460075 [info ] [Thread-1 (]: 3 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m14:33:23.463066 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d, now test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m14:33:23.464063 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:23.476031 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:33:23.485008 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 14:33:23.465060 => 14:33:23.484010
[0m14:33:23.486005 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:23.492986 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:33:23.501962 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:23.503958 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:33:23.504962 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:33:23.506948 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:24.039528 [debug] [Thread-1 (]: SQL status: OK in 0.5299999713897705 seconds
[0m14:33:24.045513 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 14:33:23.487002 => 14:33:24.045513
[0m14:33:24.047507 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m14:33:24.048507 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:24.049502 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m14:33:24.295845 [info ] [Thread-1 (]: 3 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 0.83s]
[0m14:33:24.297842 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:24.299834 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:24.300832 [info ] [Thread-1 (]: 4 of 9 START test not_null_stg_orders_customer_id .............................. [RUN]
[0m14:33:24.303823 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5)
[0m14:33:24.304821 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:24.318786 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:33:24.321778 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (compile): 14:33:24.305818 => 14:33:24.321778
[0m14:33:24.322775 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:24.328763 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:33:24.331749 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:24.332746 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:33:24.333743 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_orders`
where customer_id is null



      
    ) dbt_internal_test
[0m14:33:24.334741 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:24.929142 [debug] [Thread-1 (]: SQL status: OK in 0.6000000238418579 seconds
[0m14:33:24.936124 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (execute): 14:33:24.323773 => 14:33:24.935126
[0m14:33:24.937121 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: ROLLBACK
[0m14:33:24.938119 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:24.939116 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: Close
[0m14:33:25.097693 [info ] [Thread-1 (]: 4 of 9 PASS not_null_stg_orders_customer_id .................................... [[32mPASS[0m in 0.79s]
[0m14:33:25.099689 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:25.100688 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:25.101685 [info ] [Thread-1 (]: 5 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m14:33:25.103680 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5, now test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:33:25.104677 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:25.113650 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:25.116645 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (compile): 14:33:25.104677 => 14:33:25.116645
[0m14:33:25.117645 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:25.121632 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:25.124629 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:25.125623 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:25.128627 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `test_rw`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m14:33:25.130605 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:25.647482 [debug] [Thread-1 (]: SQL status: OK in 0.5199999809265137 seconds
[0m14:33:25.651471 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (execute): 14:33:25.118672 => 14:33:25.651471
[0m14:33:25.652468 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m14:33:25.652468 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:25.653466 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m14:33:25.829997 [info ] [Thread-1 (]: 5 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 0.73s]
[0m14:33:25.831991 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:25.832987 [debug] [Thread-1 (]: Began running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:25.833985 [info ] [Thread-1 (]: 6 of 9 START test relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [RUN]
[0m14:33:25.836976 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64, now test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500)
[0m14:33:25.837975 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:25.852583 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:33:25.855542 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (compile): 14:33:25.838972 => 14:33:25.855542
[0m14:33:25.856539 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:25.862523 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:33:25.866513 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:25.867510 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:33:25.869504 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from `test_rw`.`stg_orders`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `test_rw`.`stg_customers`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m14:33:25.870501 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:26.416046 [debug] [Thread-1 (]: SQL status: OK in 0.550000011920929 seconds
[0m14:33:26.421033 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (execute): 14:33:25.856539 => 14:33:26.420035
[0m14:33:26.422030 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: ROLLBACK
[0m14:33:26.422030 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:26.423027 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: Close
[0m14:33:26.606538 [info ] [Thread-1 (]: 6 of 9 PASS relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [[32mPASS[0m in 0.77s]
[0m14:33:26.608533 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:26.609530 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:26.610528 [info ] [Thread-1 (]: 7 of 9 START test unique_customers_customer_id ................................. [RUN]
[0m14:33:26.612523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500, now test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1)
[0m14:33:26.613520 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:26.624490 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:33:26.627483 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (compile): 14:33:26.613520 => 14:33:26.627483
[0m14:33:26.628480 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:26.632469 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:33:26.638456 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:26.639451 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:33:26.640448 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:33:26.640448 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:27.154809 [debug] [Thread-1 (]: SQL status: OK in 0.5099999904632568 seconds
[0m14:33:27.159795 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (execute): 14:33:26.628480 => 14:33:27.159795
[0m14:33:27.160795 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: ROLLBACK
[0m14:33:27.161791 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:27.162790 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: Close
[0m14:33:27.324357 [info ] [Thread-1 (]: 7 of 9 PASS unique_customers_customer_id ....................................... [[32mPASS[0m in 0.71s]
[0m14:33:27.327348 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:27.328345 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:27.329344 [info ] [Thread-1 (]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m14:33:27.331340 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1, now test.jaffle_shop.unique_stg_customers_customer_id.c7614daada)
[0m14:33:27.331340 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:27.340315 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:33:27.344308 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (compile): 14:33:27.332337 => 14:33:27.343308
[0m14:33:27.346302 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:27.353278 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:33:27.355273 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:27.356270 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:33:27.357269 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:33:27.358268 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:27.861044 [debug] [Thread-1 (]: SQL status: OK in 0.5 seconds
[0m14:33:27.865033 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (execute): 14:33:27.348293 => 14:33:27.865033
[0m14:33:27.866031 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m14:33:27.867028 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:27.867028 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m14:33:28.033582 [info ] [Thread-1 (]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 0.70s]
[0m14:33:28.034580 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:28.035577 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:28.036574 [info ] [Thread-1 (]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m14:33:28.039567 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_customers_customer_id.c7614daada, now test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a)
[0m14:33:28.040565 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:28.051536 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:28.054529 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (compile): 14:33:28.041563 => 14:33:28.054529
[0m14:33:28.055526 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:28.062506 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:28.066495 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:28.067491 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:28.068490 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_orders`
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:33:28.069487 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:28.608581 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m14:33:28.613567 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (execute): 14:33:28.056538 => 14:33:28.613567
[0m14:33:28.614564 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m14:33:28.615574 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:28.617568 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m14:33:28.778131 [info ] [Thread-1 (]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 0.74s]
[0m14:33:28.780125 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:28.782120 [debug] [MainThread]: On master: ROLLBACK
[0m14:33:28.782120 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:33:28.989566 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:33:28.990563 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:28.991560 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:33:28.992557 [debug] [MainThread]: On master: ROLLBACK
[0m14:33:28.993556 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:33:28.993556 [debug] [MainThread]: On master: Close
[0m14:33:29.172080 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:29.173078 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:33:29.174074 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:33:29.175073 [info ] [MainThread]: 
[0m14:33:29.177066 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 8.46 seconds (8.46s).
[0m14:33:29.181063 [debug] [MainThread]: Command end result
[0m14:33:29.201003 [info ] [MainThread]: 
[0m14:33:29.204016 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:33:29.205988 [info ] [MainThread]: 
[0m14:33:29.206985 [error] [MainThread]: [31mFailure in test accepted_values_stg_orders_status__placed__shipped__completed__return_pending (models\badger.yml)[0m
[0m14:33:29.207984 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:33:29.209981 [info ] [MainThread]: 
[0m14:33:29.211974 [info ] [MainThread]:   compiled Code at target\compiled\jaffle_shop\models\badger.yml\accepted_values_stg_orders_62d7846404b81142bf3afb7b27ee2a82.sql
[0m14:33:29.213970 [info ] [MainThread]: 
[0m14:33:29.215964 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=1 SKIP=0 TOTAL=9
[0m14:33:29.217958 [debug] [MainThread]: Command `dbt test` failed at 14:33:29.217958 after 11.51 seconds
[0m14:33:29.218954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736EFB2910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736A589650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001736A5895D0>]}
[0m14:33:29.220951 [debug] [MainThread]: Flushing usage events
[0m14:33:45.652977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF8B1D9050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF8B5EB110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF8B503950>]}


============================== 14:33:45.658961 | a222bd09-c418-4f4b-82ee-416f847a22da ==============================
[0m14:33:45.658961 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:33:45.660955 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:33:47.830888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a222bd09-c418-4f4b-82ee-416f847a22da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF8B4E5110>]}
[0m14:33:47.853797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a222bd09-c418-4f4b-82ee-416f847a22da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF8B97DC10>]}
[0m14:33:47.883718 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:33:48.065234 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:33:48.067237 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models\badger.yml
[0m14:33:48.109115 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_orders.sql
[0m14:33:48.260713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a222bd09-c418-4f4b-82ee-416f847a22da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF9CC42810>]}
[0m14:33:48.281683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a222bd09-c418-4f4b-82ee-416f847a22da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF9CECA390>]}
[0m14:33:48.282689 [info ] [MainThread]: Found 3 models, 9 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:33:48.284648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a222bd09-c418-4f4b-82ee-416f847a22da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF9CECBFD0>]}
[0m14:33:48.287639 [info ] [MainThread]: 
[0m14:33:48.290631 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:33:48.297613 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:33:48.329212 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:48.331228 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:33:48.333198 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:33:48.335195 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:48.924925 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m14:33:48.945844 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:33:48.946862 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:33:49.294939 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m14:33:49.300889 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:33:49.301887 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:33:49.302883 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:33:49.463455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a222bd09-c418-4f4b-82ee-416f847a22da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF9CCA0790>]}
[0m14:33:49.464453 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:49.465451 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:33:49.467445 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:49.468442 [info ] [MainThread]: 
[0m14:33:49.478417 [debug] [Thread-1 (]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:33:49.481412 [info ] [Thread-1 (]: 1 of 9 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m14:33:49.484414 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad'
[0m14:33:49.486396 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:33:49.505344 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:33:49.510332 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (compile): 14:33:49.487394 => 14:33:49.509333
[0m14:33:49.513483 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:33:49.548736 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:33:49.552725 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:49.554718 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:33:49.555715 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `test_rw`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m14:33:49.556713 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:33:50.156112 [debug] [Thread-1 (]: SQL status: OK in 0.6000000238418579 seconds
[0m14:33:50.162126 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (execute): 14:33:49.516469 => 14:33:50.162126
[0m14:33:50.164100 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: ROLLBACK
[0m14:33:50.165118 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:50.166089 [debug] [Thread-1 (]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: Close
[0m14:33:50.350594 [info ] [Thread-1 (]: 1 of 9 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 0.87s]
[0m14:33:50.353588 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:33:50.354586 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:50.354586 [info ] [Thread-1 (]: 2 of 9 START test not_null_customers_customer_id ............................... [RUN]
[0m14:33:50.357579 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad, now test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d)
[0m14:33:50.358576 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:50.376569 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:33:50.382112 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (compile): 14:33:50.358576 => 14:33:50.382112
[0m14:33:50.384104 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:50.388094 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:33:50.390087 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:50.391085 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:33:50.392083 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:33:50.393079 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:50.903653 [debug] [Thread-1 (]: SQL status: OK in 0.5099999904632568 seconds
[0m14:33:50.907675 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (execute): 14:33:50.384104 => 14:33:50.907675
[0m14:33:50.908674 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: ROLLBACK
[0m14:33:50.909638 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:50.910635 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: Close
[0m14:33:51.071584 [info ] [Thread-1 (]: 2 of 9 PASS not_null_customers_customer_id ..................................... [[32mPASS[0m in 0.72s]
[0m14:33:51.073581 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:33:51.074578 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:51.075575 [info ] [Thread-1 (]: 3 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m14:33:51.078567 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d, now test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m14:33:51.079599 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:51.091531 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:33:51.095827 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 14:33:51.080561 => 14:33:51.094306
[0m14:33:51.098822 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:51.103805 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:33:51.105801 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:51.106798 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:33:51.107795 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m14:33:51.108828 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:51.647127 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m14:33:51.651116 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 14:33:51.099820 => 14:33:51.651116
[0m14:33:51.652113 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m14:33:51.653110 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:51.654108 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m14:33:51.833314 [info ] [Thread-1 (]: 3 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 0.76s]
[0m14:33:51.835310 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:33:51.836306 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:51.837303 [info ] [Thread-1 (]: 4 of 9 START test not_null_stg_orders_customer_id .............................. [RUN]
[0m14:33:51.839299 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5)
[0m14:33:51.840307 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:51.851269 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:33:51.854274 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (compile): 14:33:51.841293 => 14:33:51.854274
[0m14:33:51.855292 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:51.859244 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:33:51.864447 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:51.866440 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"
[0m14:33:51.867437 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `test_rw`.`stg_orders`
where customer_id is null



      
    ) dbt_internal_test
[0m14:33:51.869432 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:52.384932 [debug] [Thread-1 (]: SQL status: OK in 0.5199999809265137 seconds
[0m14:33:52.388920 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5 (execute): 14:33:51.855292 => 14:33:52.388920
[0m14:33:52.389918 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: ROLLBACK
[0m14:33:52.390916 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:52.391913 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5: Close
[0m14:33:52.555475 [info ] [Thread-1 (]: 4 of 9 PASS not_null_stg_orders_customer_id .................................... [[32mPASS[0m in 0.72s]
[0m14:33:52.557472 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5
[0m14:33:52.558470 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:52.559466 [info ] [Thread-1 (]: 5 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m14:33:52.562459 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_customer_id.af79d5e4b5, now test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:33:52.563456 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:52.582165 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:52.585156 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (compile): 14:33:52.563456 => 14:33:52.584159
[0m14:33:52.586153 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:52.591142 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:52.594133 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:52.595805 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:52.598800 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `test_rw`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m14:33:52.600810 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:53.206926 [debug] [Thread-1 (]: SQL status: OK in 0.6100000143051147 seconds
[0m14:33:53.211910 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (execute): 14:33:52.587152 => 14:33:53.210913
[0m14:33:53.212907 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m14:33:53.212907 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:53.213904 [debug] [Thread-1 (]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m14:33:53.393425 [info ] [Thread-1 (]: 5 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 0.83s]
[0m14:33:53.396418 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:53.397415 [debug] [Thread-1 (]: Began running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:53.398413 [info ] [Thread-1 (]: 6 of 9 START test relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [RUN]
[0m14:33:53.400410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64, now test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500)
[0m14:33:53.401405 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:53.414450 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:33:53.419436 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (compile): 14:33:53.402402 => 14:33:53.418438
[0m14:33:53.420432 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:53.424422 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:33:53.429186 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:53.430183 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:33:53.431180 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from `test_rw`.`stg_orders`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `test_rw`.`stg_customers`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m14:33:53.433177 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:54.011340 [debug] [Thread-1 (]: SQL status: OK in 0.5799999833106995 seconds
[0m14:33:54.016327 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500 (execute): 14:33:53.421430 => 14:33:54.016327
[0m14:33:54.017324 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: ROLLBACK
[0m14:33:54.018322 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:54.019319 [debug] [Thread-1 (]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: Close
[0m14:33:54.195471 [info ] [Thread-1 (]: 6 of 9 PASS relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [[32mPASS[0m in 0.80s]
[0m14:33:54.197466 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:33:54.198464 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:54.199462 [info ] [Thread-1 (]: 7 of 9 START test unique_customers_customer_id ................................. [RUN]
[0m14:33:54.202451 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500, now test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1)
[0m14:33:54.203451 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:54.221570 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:33:54.224543 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (compile): 14:33:54.204446 => 14:33:54.223558
[0m14:33:54.225514 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:54.232217 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:33:54.235239 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:54.236235 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:33:54.237203 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:33:54.238201 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:54.790198 [debug] [Thread-1 (]: SQL status: OK in 0.550000011920929 seconds
[0m14:33:54.796154 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (execute): 14:33:54.226509 => 14:33:54.795157
[0m14:33:54.797151 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: ROLLBACK
[0m14:33:54.798148 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:54.799147 [debug] [Thread-1 (]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: Close
[0m14:33:54.962226 [info ] [Thread-1 (]: 7 of 9 PASS unique_customers_customer_id ....................................... [[32mPASS[0m in 0.76s]
[0m14:33:54.964219 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:33:54.965216 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:54.966216 [info ] [Thread-1 (]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m14:33:54.968209 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1, now test.jaffle_shop.unique_stg_customers_customer_id.c7614daada)
[0m14:33:54.968209 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:54.980179 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:33:54.983168 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (compile): 14:33:54.969208 => 14:33:54.983168
[0m14:33:54.984165 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:54.991156 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:33:54.995144 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:54.997131 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:33:54.998129 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_customers`
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:33:55.000125 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:55.558080 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m14:33:55.564063 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (execute): 14:33:54.985165 => 14:33:55.563065
[0m14:33:55.565060 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m14:33:55.566057 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:55.568052 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m14:33:55.763531 [info ] [Thread-1 (]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 0.80s]
[0m14:33:55.766527 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:33:55.767520 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:55.768519 [info ] [Thread-1 (]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m14:33:55.771510 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_customers_customer_id.c7614daada, now test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a)
[0m14:33:55.772509 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:55.787469 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:55.790461 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (compile): 14:33:55.773507 => 14:33:55.789464
[0m14:33:55.791459 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:55.801430 [debug] [Thread-1 (]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:55.805420 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:55.807414 [debug] [Thread-1 (]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:55.808414 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from `test_rw`.`stg_orders`
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:33:55.811415 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:56.413110 [debug] [Thread-1 (]: SQL status: OK in 0.6000000238418579 seconds
[0m14:33:56.418096 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (execute): 14:33:55.792453 => 14:33:56.417099
[0m14:33:56.419093 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m14:33:56.420091 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:33:56.421088 [debug] [Thread-1 (]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m14:33:56.591633 [info ] [Thread-1 (]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 0.82s]
[0m14:33:56.594626 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:33:56.597618 [debug] [MainThread]: On master: ROLLBACK
[0m14:33:56.598616 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:33:56.828374 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:33:56.829364 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:33:56.830332 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:33:56.831361 [debug] [MainThread]: On master: ROLLBACK
[0m14:33:56.832328 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:33:56.833324 [debug] [MainThread]: On master: Close
[0m14:33:57.011849 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:57.013843 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:33:57.014840 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:33:57.016835 [info ] [MainThread]: 
[0m14:33:57.018831 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 8.73 seconds (8.73s).
[0m14:33:57.022821 [debug] [MainThread]: Command end result
[0m14:33:57.055048 [info ] [MainThread]: 
[0m14:33:57.057042 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:33:57.060035 [info ] [MainThread]: 
[0m14:33:57.065565 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m14:33:57.069542 [debug] [MainThread]: Command `dbt test` succeeded at 14:33:57.068545 after 11.49 seconds
[0m14:33:57.070539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF8B284A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF8660F450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF86899710>]}
[0m14:33:57.071537 [debug] [MainThread]: Flushing usage events
[0m14:35:08.721641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03BF708D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03BF49FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03C2F5810>]}


============================== 14:35:08.726635 | bd856c2c-ae21-47bf-9e5e-b45f2da604c8 ==============================
[0m14:35:08.726635 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:35:08.728595 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:35:10.863904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bd856c2c-ae21-47bf-9e5e-b45f2da604c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03C6BE310>]}
[0m14:35:10.888835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bd856c2c-ae21-47bf-9e5e-b45f2da604c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E04D9CE610>]}
[0m14:35:10.917760 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:35:11.108284 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m14:35:11.109281 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models\schema.yml
[0m14:35:11.145152 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_customers.sql
[0m14:35:11.170121 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m14:35:11.177104 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_orders.sql
[0m14:35:11.306749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd856c2c-ae21-47bf-9e5e-b45f2da604c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E04DAB8E10>]}
[0m14:35:11.313702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd856c2c-ae21-47bf-9e5e-b45f2da604c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E04DAC7C10>]}
[0m14:35:11.314699 [info ] [MainThread]: Found 3 models, 7 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:35:11.316694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd856c2c-ae21-47bf-9e5e-b45f2da604c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E04DAA80D0>]}
[0m14:35:11.318689 [info ] [MainThread]: 
[0m14:35:11.321683 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:35:11.326669 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_test_rw'
[0m14:35:11.354593 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:11.355591 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:35:11.356588 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show tables in `test_rw`
  
[0m14:35:11.357585 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:12.064381 [debug] [ThreadPool]: SQL status: OK in 0.7099999785423279 seconds
[0m14:35:12.082335 [debug] [ThreadPool]: Using databricks connection "list_None_test_rw"
[0m14:35:12.083332 [debug] [ThreadPool]: On list_None_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_test_rw"} */
show views in `test_rw`
  
[0m14:35:12.474329 [debug] [ThreadPool]: SQL status: OK in 0.38999998569488525 seconds
[0m14:35:12.480273 [debug] [ThreadPool]: On list_None_test_rw: ROLLBACK
[0m14:35:12.480273 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:35:12.481270 [debug] [ThreadPool]: On list_None_test_rw: Close
[0m14:35:12.643836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd856c2c-ae21-47bf-9e5e-b45f2da604c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E04DAA80D0>]}
[0m14:35:12.646871 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:12.647826 [info ] [MainThread]: 
[0m14:35:12.654807 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_customers
[0m14:35:12.656802 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.stg_customers'
[0m14:35:12.657800 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_customers
[0m14:35:12.664781 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m14:35:12.669772 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (compile): 14:35:12.658797 => 14:35:12.668778
[0m14:35:12.671767 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_customers
[0m14:35:12.672789 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (execute): 14:35:12.672789 => 14:35:12.672789
[0m14:35:12.674789 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_customers
[0m14:35:12.675757 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_orders
[0m14:35:12.676783 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_customers, now model.jaffle_shop.stg_orders)
[0m14:35:12.676783 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_orders
[0m14:35:12.684755 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m14:35:12.686750 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (compile): 14:35:12.677748 => 14:35:12.686750
[0m14:35:12.687749 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_orders
[0m14:35:12.688719 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (execute): 14:35:12.687749 => 14:35:12.687749
[0m14:35:12.690714 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_orders
[0m14:35:12.691721 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:35:12.695700 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_orders, now test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m14:35:12.697698 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:35:12.717649 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:35:12.720635 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 14:35:12.699692 => 14:35:12.720635
[0m14:35:12.721631 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:35:12.722628 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 14:35:12.722628 => 14:35:12.722628
[0m14:35:12.724626 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:35:12.725621 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:35:12.726618 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop.unique_stg_customers_customer_id.c7614daada)
[0m14:35:12.726618 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:35:12.738584 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:35:12.740578 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (compile): 14:35:12.727615 => 14:35:12.739581
[0m14:35:12.741576 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:35:12.742573 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (execute): 14:35:12.741576 => 14:35:12.741576
[0m14:35:12.743571 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:35:12.744570 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:35:12.746566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_customers_customer_id.c7614daada, now model.jaffle_shop.customers)
[0m14:35:12.747562 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:35:12.751551 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:35:12.753547 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:35:12.747562 => 14:35:12.753547
[0m14:35:12.754544 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:35:12.755541 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:35:12.755541 => 14:35:12.755541
[0m14:35:12.756538 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:35:12.757536 [debug] [Thread-1 (]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:35:12.759528 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad)
[0m14:35:12.760525 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:35:12.771496 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:35:12.773491 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (compile): 14:35:12.760525 => 14:35:12.773491
[0m14:35:12.774490 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:35:12.776483 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (execute): 14:35:12.775485 => 14:35:12.775485
[0m14:35:12.778478 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:35:12.779475 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:35:12.781469 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad, now test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:35:12.782467 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:35:12.789448 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:35:12.791445 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (compile): 14:35:12.783464 => 14:35:12.791445
[0m14:35:12.792441 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:35:12.793437 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (execute): 14:35:12.792441 => 14:35:12.793437
[0m14:35:12.794435 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:35:12.796435 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:35:12.797428 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64, now test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a)
[0m14:35:12.798424 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:35:12.804409 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:35:12.806403 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (compile): 14:35:12.799422 => 14:35:12.806403
[0m14:35:12.807400 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:35:12.808398 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (execute): 14:35:12.808398 => 14:35:12.808398
[0m14:35:12.810393 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:35:12.810393 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:35:12.812388 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a, now test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d)
[0m14:35:12.813385 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:35:12.819368 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:35:12.821363 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (compile): 14:35:12.813385 => 14:35:12.820366
[0m14:35:12.822398 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:35:12.823395 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (execute): 14:35:12.823395 => 14:35:12.823395
[0m14:35:12.825391 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:35:12.826386 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:35:12.828380 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d, now test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1)
[0m14:35:12.829342 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:35:12.835361 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:35:12.837323 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (compile): 14:35:12.829342 => 14:35:12.837323
[0m14:35:12.838322 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:35:12.839318 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (execute): 14:35:12.839318 => 14:35:12.839318
[0m14:35:12.841312 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:35:12.842309 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:12.843307 [debug] [MainThread]: Connection 'list_None_test_rw' was properly closed.
[0m14:35:12.843307 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1' was properly closed.
[0m14:35:12.847296 [debug] [MainThread]: Command end result
[0m14:35:12.874223 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m14:35:12.875248 [info ] [MainThread]: Building catalog
[0m14:35:12.878214 [debug] [ThreadPool]: Acquiring new databricks connection 'test_rw'
[0m14:35:12.959994 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:12.960991 [debug] [ThreadPool]: Using databricks connection "test_rw"
[0m14:35:12.961989 [debug] [ThreadPool]: On test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "test_rw"} */
show table extended in `test_rw` like 'stg_customers|customers|stg_orders'
  
[0m14:35:12.962986 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:13.652139 [debug] [ThreadPool]: SQL status: OK in 0.6899999976158142 seconds
[0m14:35:13.656128 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `test_rw`.`customers`
[0m14:35:13.657125 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `test_rw`.`stg_customers`
[0m14:35:13.658124 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `test_rw`.`stg_orders`
[0m14:35:13.661117 [debug] [ThreadPool]: On test_rw: ROLLBACK
[0m14:35:13.662115 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:35:13.663111 [debug] [ThreadPool]: On test_rw: Close
[0m14:35:13.838524 [info ] [MainThread]: Catalog written to C:\Git\dbt-tutorial\jaffle_shop\target\catalog.json
[0m14:35:13.841482 [debug] [MainThread]: Command `dbt docs generate` succeeded at 14:35:13.840485 after 5.18 seconds
[0m14:35:13.841482 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m14:35:13.842480 [debug] [MainThread]: Connection 'test_rw' was properly closed.
[0m14:35:13.843477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03BF85B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03BF87950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E037638E10>]}
[0m14:35:13.844474 [debug] [MainThread]: Flushing usage events
[0m14:35:31.873813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234E0189A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234DFE75A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234E00A1950>]}


============================== 14:35:31.881449 | afad0710-fdbc-4496-be0b-9b49b660fc14 ==============================
[0m14:35:31.881449 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:35:31.883564 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:35:34.548452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'afad0710-fdbc-4496-be0b-9b49b660fc14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234E05BFBD0>]}
[0m14:35:34.572387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'afad0710-fdbc-4496-be0b-9b49b660fc14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234F18B1010>]}
[0m14:35:34.607330 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:35:34.801778 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:34.802775 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:34.814740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'afad0710-fdbc-4496-be0b-9b49b660fc14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234F1898C10>]}
[0m14:38:50.985178 [error] [MainThread]: Encountered an error:

[0m14:38:50.995334 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\cli\requires.py", line 86, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\cli\requires.py", line 71, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\cli\requires.py", line 142, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\cli\requires.py", line 215, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\cli\requires.py", line 250, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\cli\main.py", line 301, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\site-packages\dbt\task\serve.py", line 28, in run
    httpd.serve_forever()
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\socketserver.py", line 233, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 323, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\RWillock\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 314, in _select
    r, w, x = select.select(r, w, w, timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m14:38:51.005334 [debug] [MainThread]: Command `dbt docs serve` failed at 14:38:51.005334 after 199.22 seconds
[0m14:38:51.006299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234E00A1950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234E05AD590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234DFE5DE10>]}
[0m14:38:51.007326 [debug] [MainThread]: Flushing usage events
[0m14:44:36.627524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ADE41E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ADB70A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ADB625D0>]}


============================== 14:44:36.632509 | 93be74e7-6c38-48b5-b1f8-52d185451b8f ==============================
[0m14:44:36.632509 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:44:36.634505 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:44:38.022793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '93be74e7-6c38-48b5-b1f8-52d185451b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198AE2BA910>]}
[0m14:44:38.048724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '93be74e7-6c38-48b5-b1f8-52d185451b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198BF582CD0>]}
[0m14:44:38.081634 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:44:38.099586 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m14:44:38.100583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '93be74e7-6c38-48b5-b1f8-52d185451b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198BF6D0F10>]}
[0m14:44:39.219592 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m14:44:39.243566 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_customers.sql
[0m14:44:39.248553 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_orders.sql
[0m14:44:39.376173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93be74e7-6c38-48b5-b1f8-52d185451b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198BF682210>]}
[0m14:44:39.380369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93be74e7-6c38-48b5-b1f8-52d185451b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198BF687ED0>]}
[0m14:44:39.380369 [info ] [MainThread]: Found 3 models, 7 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:44:39.382364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93be74e7-6c38-48b5-b1f8-52d185451b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198BF6AE1D0>]}
[0m14:44:39.384359 [info ] [MainThread]: 
[0m14:44:39.387351 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:44:39.391341 [debug] [ThreadPool]: Acquiring new databricks connection 'list_badger_test_rw'
[0m14:44:39.400351 [debug] [ThreadPool]: Using databricks connection "list_badger_test_rw"
[0m14:44:39.401351 [debug] [ThreadPool]: On list_badger_test_rw: GetTables(database=badger, schema=test_rw, identifier=None)
[0m14:44:39.401351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:39.997186 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m14:44:40.003172 [debug] [ThreadPool]: On list_badger_test_rw: Close
[0m14:44:40.190133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93be74e7-6c38-48b5-b1f8-52d185451b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198BF6AE1D0>]}
[0m14:44:40.191131 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:44:40.191731 [info ] [MainThread]: 
[0m14:44:40.198716 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_customers
[0m14:44:40.201709 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.stg_customers'
[0m14:44:40.202705 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_customers
[0m14:44:40.205859 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m14:44:40.210684 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (compile): 14:44:40.202705 => 14:44:40.209687
[0m14:44:40.211681 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_customers
[0m14:44:40.213676 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (execute): 14:44:40.213676 => 14:44:40.213676
[0m14:44:40.215669 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_customers
[0m14:44:40.215669 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_orders
[0m14:44:40.217664 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_customers, now model.jaffle_shop.stg_orders)
[0m14:44:40.217664 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_orders
[0m14:44:40.220690 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m14:44:40.223650 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (compile): 14:44:40.218661 => 14:44:40.223650
[0m14:44:40.225672 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_orders
[0m14:44:40.226640 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (execute): 14:44:40.226640 => 14:44:40.226640
[0m14:44:40.228670 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_orders
[0m14:44:40.229632 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:40.231627 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_orders, now test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m14:44:40.231627 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:40.248617 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:44:40.250617 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 14:44:40.232625 => 14:44:40.250617
[0m14:44:40.251608 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:40.252607 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 14:44:40.251608 => 14:44:40.252607
[0m14:44:40.253603 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:40.254566 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:44:40.257591 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop.unique_stg_customers_customer_id.c7614daada)
[0m14:44:40.258556 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:44:40.268562 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:44:40.270559 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (compile): 14:44:40.260590 => 14:44:40.270559
[0m14:44:40.271520 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:44:40.273516 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (execute): 14:44:40.272520 => 14:44:40.272520
[0m14:44:40.275544 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:44:40.275544 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:44:40.276542 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_customers_customer_id.c7614daada, now model.jaffle_shop.customers)
[0m14:44:40.277538 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:44:40.281536 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:44:40.282523 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:44:40.278537 => 14:44:40.282523
[0m14:44:40.283522 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:44:40.284521 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:44:40.283522 => 14:44:40.283522
[0m14:44:40.285517 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:44:40.286514 [debug] [Thread-1 (]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:44:40.287433 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad)
[0m14:44:40.288433 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:44:40.296449 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:44:40.298406 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (compile): 14:44:40.289431 => 14:44:40.298406
[0m14:44:40.299403 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:44:40.300400 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (execute): 14:44:40.300400 => 14:44:40.300400
[0m14:44:40.302400 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:44:40.302400 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:44:40.303394 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad, now test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:44:40.304389 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:44:40.309376 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:44:40.311370 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (compile): 14:44:40.304389 => 14:44:40.311370
[0m14:44:40.312368 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:44:40.313365 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (execute): 14:44:40.313365 => 14:44:40.313365
[0m14:44:40.314363 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:44:40.315360 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:44:40.316357 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64, now test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a)
[0m14:44:40.316357 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:44:40.323339 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:44:40.325334 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (compile): 14:44:40.317355 => 14:44:40.325334
[0m14:44:40.326330 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:44:40.327328 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (execute): 14:44:40.326330 => 14:44:40.326330
[0m14:44:40.328325 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:44:40.328325 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:44:40.329323 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a, now test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d)
[0m14:44:40.330320 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:44:40.335307 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:44:40.337301 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (compile): 14:44:40.330320 => 14:44:40.337301
[0m14:44:40.338300 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:44:40.339299 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (execute): 14:44:40.338300 => 14:44:40.338300
[0m14:44:40.340295 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:44:40.341292 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:44:40.342288 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d, now test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1)
[0m14:44:40.342288 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:44:40.347274 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:44:40.349269 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (compile): 14:44:40.343285 => 14:44:40.348272
[0m14:44:40.349269 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:44:40.350267 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (execute): 14:44:40.350267 => 14:44:40.350267
[0m14:44:40.351264 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:44:40.353129 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:44:40.353129 [debug] [MainThread]: Connection 'list_badger_test_rw' was properly closed.
[0m14:44:40.354168 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1' was properly closed.
[0m14:44:40.357157 [debug] [MainThread]: Command end result
[0m14:44:40.377109 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m14:44:40.378105 [info ] [MainThread]: Building catalog
[0m14:44:40.380062 [debug] [ThreadPool]: Acquiring new databricks connection 'test_rw'
[0m14:44:40.395055 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:40.396059 [debug] [ThreadPool]: Using databricks connection "test_rw"
[0m14:44:40.397015 [debug] [ThreadPool]: On test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "test_rw"} */

      select current_catalog()
  
[0m14:44:40.397015 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:40.892261 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "test_rw"} */

      select current_catalog()
  
[0m14:44:40.893249 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog 'badger' plugin class not found: spark.sql.catalog.badger is not defined
[0m14:44:40.894245 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'badger' plugin class not found: spark.sql.catalog.badger is not defined
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:609)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:501)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:361)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:339)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:324)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:373)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'badger' plugin class not found: spark.sql.catalog.badger is not defined
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundError(QueryExecutionErrors.scala:2210)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:63)
	at org.apache.spark.sql.connector.catalog.SecureCatalogManager.assertCatalogLoadable(SecureCatalogManager.scala:44)
	at org.apache.spark.sql.connector.catalog.SecureCatalogManager.assertCatalogLoadable$(SecureCatalogManager.scala:40)
	at org.apache.spark.sql.connector.catalog.CatalogManager.assertCatalogLoadable(CatalogManager.scala:40)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	at com.databricks.sql.DatabricksCatalogManager.catalog(DatabricksCatalogManager.scala:96)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:127)
	at com.databricks.sql.DatabricksCatalogManager.currentCatalog(DatabricksCatalogManager.scala:114)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:98)
	at com.databricks.sql.DatabricksCatalogManager.currentNamespace(DatabricksCatalogManager.scala:147)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:110)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:398)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:398)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:374)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:297)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:293)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:289)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:307)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:326)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:323)
	at org.apache.spark.sql.execution.QueryExecution.assertExecutedPlanPrepared(QueryExecution.scala:345)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.createExecutionPlanAndReportEvent(ResultCollector.scala:87)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.createExecutionPlanAndReportEvent$(ResultCollector.scala:77)
	at org.apache.spark.sql.hive.thriftserver.ArrowResultHandler.createExecutionPlanAndReportEvent(ArrowResultHandler.scala:32)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult(ResultCollector.scala:67)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult$(ResultCollector.scala:31)
	at org.apache.spark.sql.hive.thriftserver.ArrowResultHandler.collectResult(ArrowResultHandler.scala:32)
	at org.apache.spark.sql.hive.thriftserver.ArrowResultHandler$$anon$1.iterator(ArrowResultHandler.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ArrowFetchIterator.<init>(ArrowFetchIterator.scala:24)
	at org.apache.spark.sql.hive.thriftserver.ArrowResultHandler.org$apache$spark$sql$hive$thriftserver$ArrowResultHandler$$initFromDataFrame(ArrowResultHandler.scala:61)
	at org.apache.spark.sql.hive.thriftserver.ArrowResultHandler$.createFromDataFrame(ArrowResultHandler.scala:189)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.createResultHandler(ResultHandlerFactory.scala:343)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:564)
	... 20 more

[0m14:44:40.895279 [debug] [ThreadPool]: Databricks adapter: operation-id: b'\x01\xed\xf5\x82#n\x1b\x0b\x99]\x1c\xbbpUW"'
[0m14:44:40.896240 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro current_catalog
[0m14:44:40.896240 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Catalog 'badger' plugin class not found: spark.sql.catalog.badger is not defined
[0m14:44:40.897237 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about `badger`.`test_rw`: Runtime Error
  Catalog 'badger' plugin class not found: spark.sql.catalog.badger is not defined
[0m14:44:40.898268 [debug] [ThreadPool]: On test_rw: ROLLBACK
[0m14:44:40.898268 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:44:40.899265 [debug] [ThreadPool]: On test_rw: Close
[0m14:44:41.064867 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    Catalog 'badger' plugin class not found: spark.sql.catalog.badger is not defined
[0m14:44:41.082418 [error] [MainThread]: dbt encountered 1 failure while writing the catalog
[0m14:44:41.083372 [info ] [MainThread]: Catalog written to C:\Git\dbt-tutorial\jaffle_shop\target\catalog.json
[0m14:44:41.085407 [debug] [MainThread]: Command `dbt docs generate` failed at 14:44:41.085407 after 4.51 seconds
[0m14:44:41.085407 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m14:44:41.086400 [debug] [MainThread]: Connection 'test_rw' was properly closed.
[0m14:44:41.086400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ADF0F950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198BF7CC210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9208150>]}
[0m14:44:41.087401 [debug] [MainThread]: Flushing usage events
[0m14:45:18.411349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F6DE4E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F6E9CC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F730FAD0>]}


============================== 14:45:18.415352 | 00099b5a-1037-4fcc-a8ae-d808c03df6cf ==============================
[0m14:45:18.415352 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:45:18.416072 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:45:20.244916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '00099b5a-1037-4fcc-a8ae-d808c03df6cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F6E9B290>]}
[0m14:45:20.271844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '00099b5a-1037-4fcc-a8ae-d808c03df6cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4885F3B50>]}
[0m14:45:20.303761 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:45:20.323705 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m14:45:20.325702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '00099b5a-1037-4fcc-a8ae-d808c03df6cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A48873D750>]}
[0m14:45:21.935395 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m14:45:21.958334 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_customers.sql
[0m14:45:21.967310 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_orders.sql
[0m14:45:22.156806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '00099b5a-1037-4fcc-a8ae-d808c03df6cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A488662090>]}
[0m14:45:22.161836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '00099b5a-1037-4fcc-a8ae-d808c03df6cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A48867FA90>]}
[0m14:45:22.162828 [info ] [MainThread]: Found 3 models, 7 tests, 0 snapshots, 0 analyses, 409 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m14:45:22.163567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00099b5a-1037-4fcc-a8ae-d808c03df6cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A488663390>]}
[0m14:45:22.166564 [info ] [MainThread]: 
[0m14:45:22.168562 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:45:22.173557 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_test_rw'
[0m14:45:22.181523 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_test_rw"
[0m14:45:22.182521 [debug] [ThreadPool]: On list_hive_metastore_test_rw: GetTables(database=hive_metastore, schema=test_rw, identifier=None)
[0m14:45:22.183520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:22.915317 [debug] [ThreadPool]: SQL status: OK in 0.7300000190734863 seconds
[0m14:45:22.932306 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:22.933303 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_test_rw"
[0m14:45:22.934268 [debug] [ThreadPool]: On list_hive_metastore_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_hive_metastore_test_rw"} */

      select current_catalog()
  
[0m14:45:23.250140 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m14:45:23.257955 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_test_rw"
[0m14:45:23.258953 [debug] [ThreadPool]: On list_hive_metastore_test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_hive_metastore_test_rw"} */
show views in `hive_metastore`.`test_rw`
  
[0m14:45:23.658943 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m14:45:23.663928 [debug] [ThreadPool]: On list_hive_metastore_test_rw: ROLLBACK
[0m14:45:23.663928 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:45:23.664926 [debug] [ThreadPool]: On list_hive_metastore_test_rw: Close
[0m14:45:23.830499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00099b5a-1037-4fcc-a8ae-d808c03df6cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F4A36B90>]}
[0m14:45:23.832496 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:45:23.834490 [info ] [MainThread]: 
[0m14:45:23.847454 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_customers
[0m14:45:23.851447 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jaffle_shop.stg_customers'
[0m14:45:23.853438 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_customers
[0m14:45:23.860423 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m14:45:23.864409 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (compile): 14:45:23.854435 => 14:45:23.864409
[0m14:45:23.866403 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_customers
[0m14:45:23.867400 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_customers (execute): 14:45:23.866403 => 14:45:23.866403
[0m14:45:23.871393 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_customers
[0m14:45:23.875384 [debug] [Thread-1 (]: Began running node model.jaffle_shop.stg_orders
[0m14:45:23.879375 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_customers, now model.jaffle_shop.stg_orders)
[0m14:45:23.881363 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.stg_orders
[0m14:45:23.886350 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m14:45:23.891337 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (compile): 14:45:23.883358 => 14:45:23.888344
[0m14:45:23.894336 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.stg_orders
[0m14:45:23.897326 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.stg_orders (execute): 14:45:23.896323 => 14:45:23.896323
[0m14:45:23.899320 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.stg_orders
[0m14:45:23.901312 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:45:23.902308 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.stg_orders, now test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m14:45:23.903305 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:45:23.920260 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:45:23.926243 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 14:45:23.904303 => 14:45:23.924259
[0m14:45:23.927240 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:45:23.928237 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 14:45:23.928237 => 14:45:23.928237
[0m14:45:23.930232 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:45:23.931266 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:23.932228 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop.unique_stg_customers_customer_id.c7614daada)
[0m14:45:23.933225 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:23.944195 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:45:23.946192 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (compile): 14:45:23.934223 => 14:45:23.946192
[0m14:45:23.947189 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:23.948190 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_customers_customer_id.c7614daada (execute): 14:45:23.948190 => 14:45:23.948190
[0m14:45:23.949184 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:23.950181 [debug] [Thread-1 (]: Began running node model.jaffle_shop.customers
[0m14:45:23.951178 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_customers_customer_id.c7614daada, now model.jaffle_shop.customers)
[0m14:45:23.952175 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop.customers
[0m14:45:23.956164 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m14:45:23.959158 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (compile): 14:45:23.952175 => 14:45:23.959158
[0m14:45:23.960154 [debug] [Thread-1 (]: Began executing node model.jaffle_shop.customers
[0m14:45:23.961151 [debug] [Thread-1 (]: Timing info for model.jaffle_shop.customers (execute): 14:45:23.961151 => 14:45:23.961151
[0m14:45:23.963145 [debug] [Thread-1 (]: Finished running node model.jaffle_shop.customers
[0m14:45:23.964142 [debug] [Thread-1 (]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:45:23.965140 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop.customers, now test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad)
[0m14:45:23.966136 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:45:23.978104 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m14:45:23.981096 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (compile): 14:45:23.966136 => 14:45:23.980098
[0m14:45:23.981096 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:45:23.982093 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad (execute): 14:45:23.982093 => 14:45:23.982093
[0m14:45:23.984088 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m14:45:23.985087 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:23.986083 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad, now test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:45:23.987080 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:23.994063 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:45:23.996056 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (compile): 14:45:23.987080 => 14:45:23.995058
[0m14:45:23.996056 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:23.997054 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64 (execute): 14:45:23.997054 => 14:45:23.997054
[0m14:45:23.999048 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:24.000045 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:24.001042 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64, now test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a)
[0m14:45:24.001042 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:24.008024 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:45:24.011018 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (compile): 14:45:24.002039 => 14:45:24.010023
[0m14:45:24.012015 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:24.012015 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a (execute): 14:45:24.012015 => 14:45:24.012015
[0m14:45:24.014009 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:24.015005 [debug] [Thread-1 (]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:45:24.016003 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a, now test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d)
[0m14:45:24.016003 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:45:24.022988 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m14:45:24.025978 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (compile): 14:45:24.017000 => 14:45:24.024980
[0m14:45:24.025978 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:45:24.027000 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d (execute): 14:45:24.027000 => 14:45:24.027000
[0m14:45:24.028970 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m14:45:24.029966 [debug] [Thread-1 (]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:45:24.030964 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d, now test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1)
[0m14:45:24.030964 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:45:24.036950 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m14:45:24.040936 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (compile): 14:45:24.031961 => 14:45:24.039939
[0m14:45:24.041933 [debug] [Thread-1 (]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:45:24.041933 [debug] [Thread-1 (]: Timing info for test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1 (execute): 14:45:24.041933 => 14:45:24.041933
[0m14:45:24.043955 [debug] [Thread-1 (]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m14:45:24.045924 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:45:24.045924 [debug] [MainThread]: Connection 'list_hive_metastore_test_rw' was properly closed.
[0m14:45:24.046921 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1' was properly closed.
[0m14:45:24.049912 [debug] [MainThread]: Command end result
[0m14:45:24.073854 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m14:45:24.073854 [info ] [MainThread]: Building catalog
[0m14:45:24.077837 [debug] [ThreadPool]: Acquiring new databricks connection 'test_rw'
[0m14:45:24.082824 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:24.082824 [debug] [ThreadPool]: Using databricks connection "test_rw"
[0m14:45:24.083857 [debug] [ThreadPool]: On test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "test_rw"} */

      select current_catalog()
  
[0m14:45:24.084848 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:24.591234 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m14:45:24.603201 [debug] [ThreadPool]: Using databricks connection "test_rw"
[0m14:45:24.604197 [debug] [ThreadPool]: On test_rw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "2.5.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "test_rw"} */
show table extended in `hive_metastore`.`test_rw` like 'stg_orders|customers|stg_customers'
  
[0m14:45:25.084750 [debug] [ThreadPool]: SQL status: OK in 0.47999998927116394 seconds
[0m14:45:25.089737 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`test_rw`.`customers`
[0m14:45:25.090735 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`test_rw`.`stg_customers`
[0m14:45:25.091729 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`test_rw`.`stg_orders`
[0m14:45:25.094724 [debug] [ThreadPool]: On test_rw: ROLLBACK
[0m14:45:25.095721 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:45:25.096718 [debug] [ThreadPool]: On test_rw: Close
[0m14:45:25.280227 [info ] [MainThread]: Catalog written to C:\Git\dbt-tutorial\jaffle_shop\target\catalog.json
[0m14:45:25.283217 [debug] [MainThread]: Command `dbt docs generate` succeeded at 14:45:25.282220 after 6.92 seconds
[0m14:45:25.283217 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m14:45:25.284214 [debug] [MainThread]: Connection 'test_rw' was properly closed.
[0m14:45:25.285212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F7319A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F730FAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4F64796D0>]}
[0m14:45:25.285212 [debug] [MainThread]: Flushing usage events
[0m14:45:34.255363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219E4838BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219E4529150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219E1C1D5D0>]}


============================== 14:45:34.260349 | 5cd2050c-b41c-4ead-96bc-d731d70bce93 ==============================
[0m14:45:34.260349 [info ] [MainThread]: Running with dbt=1.5.0
[0m14:45:34.262344 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\RWillock\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Git\\dbt-tutorial\\jaffle_shop\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:45:36.189187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5cd2050c-b41c-4ead-96bc-d731d70bce93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219E4CAF650>]}
[0m14:45:36.215118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5cd2050c-b41c-4ead-96bc-d731d70bce93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219F5FB8710>]}
[0m14:45:36.250024 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m14:45:36.460490 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:45:36.461490 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
